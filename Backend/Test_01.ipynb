{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbfbbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration Reddit:\n",
      "Client ID: KrR4WKqaRf...\n",
      "Client Secret: wnsAK-PrFa...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configuration Reddit\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = \"RedditScraper/1.0\"\n",
    "\n",
    "print(\"üîß Configuration Reddit:\")\n",
    "print(f\"Client ID: {REDDIT_CLIENT_ID[:10]}...\" if REDDIT_CLIENT_ID else \"‚ùå Non d√©fini\")\n",
    "print(f\"Client Secret: {REDDIT_CLIENT_SECRET[:10]}...\" if REDDIT_CLIENT_SECRET else \"‚ùå Non d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1244a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, function_tool, trace, WebSearchTool\n",
    "# Cr√©er le client OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41a6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(name=\"Assistant\", instructions=\"Tu es un manager de projet intelligent, structur√©, qui comprend les besoins et les d√©pendances entre agents.\\\n",
    "    Tu comprends les besoins d'un projet et propose une bonne ou meilleure structuration\",\n",
    "           model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091d116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"OK, maintenant je te donne l'architecture compl√®te de ce que je veux faire. Vois des choses √† am√©liorer?\n",
    "\n",
    "des pompts pour les agents ? Une meilleur structuration ? Tes instructions seront ensuite donn√©es √† Claude\n",
    "afin qu'il code ce que tu dis. \n",
    "\n",
    "<d√©but>\n",
    "\n",
    "Le projet doit √™tre capable de g√©rer les demandes dans toutes les langues.\n",
    "\n",
    "Il y aura 4 agents.\n",
    "\n",
    "La structure de Reddit est ainsi : Reddit ‚Üí Subreddit ‚Üí Post ‚Üí Commentaire.\n",
    "\n",
    "Le premier sera celui en contact direct avec l'utilisateur. Il devra √™tre poli, dire ce qu‚Äôil fait, c‚Äôest-√†-dire chercher les points de douleur/friction sur les subreddits afin de trouver des solutions/id√©es, et toujours rediriger vers son objectif, √† savoir : quel subreddit scraper, combien de posts, combien de commentaires par post, ainsi que quels crit√®res parmi top, new, best, rising, hot. Si l'utilisateur ne sait pas, il doit lui expliquer ce que sont ces crit√®res et comment ils sont calcul√©s.\n",
    "\n",
    "Il aura deux outils : un pour la recherche internet au cas o√π, et un autre pour v√©rifier si le subreddit existe.\n",
    "Si oui, c‚Äôest bon. Sinon, il redirigera soit :\n",
    "\n",
    "en cas de faute d‚Äôorthographe, vers le subreddit correctement orthographi√© ;\n",
    "\n",
    "sinon, vers un subreddit proche de ce que l'utilisateur voulait ;\n",
    "\n",
    "et par d√©faut, vers les subreddits les plus courants pour les entrepreneurs.\n",
    "\n",
    "Il doit pr√©venir l'utilisateur et lui indiquer ses param√®tres par d√©faut, qui sont : 10 posts, 10 commentaires, et le crit√®re top. Avant de passer √† l'√©tape suivante, il doit s‚Äôassurer que l'utilisateur n‚Äôa rien √† ajouter, en posant une derni√®re question.\n",
    "\n",
    "L'√©tape suivante est un handoff : il transmet les crit√®res et le subreddit.\n",
    "Le second agent est un scraper. Il scrape tout selon les crit√®res, en utilisant un outil d√©di√©. Une fois qu‚Äôil a termin√©, il effectue un handoff vers un troisi√®me agent.\n",
    "\n",
    "Ce troisi√®me agent est sp√©cialis√© dans les r√©sum√©s, l‚Äôanalyse de sentiment et le scoring.\n",
    "\n",
    "Tu es un expert analyste sp√©cialis√© dans l‚Äôidentification des probl√®mes et frustrations r√©currents des utilisateurs.\n",
    "\n",
    "Ton r√¥le est de :\n",
    "\n",
    "Analyser le sentiment et l‚Äôintensit√© √©motionnelle de chaque post/commentaire ;\n",
    "\n",
    "Identifier les douleurs r√©currentes, quelle que soit la langue ;\n",
    "\n",
    "Calculer un score de priorit√© pour chaque douleur ;\n",
    "\n",
    "Classer les douleurs par ordre de priorit√© ;\n",
    "\n",
    "Lister les 5 principales douleurs sous forme d‚Äôune phrase chacune dans ton r√©sum√© final.\n",
    "\n",
    "Instructions sp√©cifiques :\n",
    "\n",
    "Analyse le sentiment principal (ex. : frustration, col√®re, stress, confusion, etc.) ‚Äî une √©motion principale par commentaire ;\n",
    "\n",
    "√âvalue l‚Äôintensit√© √©motionnelle sur une √©chelle de 1 √† 10 (10 = tr√®s intense) ;\n",
    "\n",
    "Identifie le type de probl√®me (technique, financier, relationnel, etc.) ;\n",
    "\n",
    "Regroupe les douleurs similaires ;\n",
    "\n",
    "Calcule les m√©triques associ√©es √† chaque douleur ;\n",
    "\n",
    "Pr√©sente les 3 douleurs principales avec leur score.\n",
    "\n",
    "Format de sortie attendu :\n",
    "\n",
    "Liste jusqu‚Äô√† 10 principales douleurs/probl√®mes rencontr√©s (une phrase chacune) ;\n",
    "\n",
    "Top 3 douleurs class√©es par score ;\n",
    "\n",
    "R√©sum√© en un paragraphe ;\n",
    "\n",
    "Si des commentaires sont particuli√®rement int√©ressants car ils proposent explicitement une solution, garde-les en m√©moire et demande √† l'utilisateur s‚Äôil souhaite les lire ou obtenir les liens.\n",
    "\n",
    "Enfin, il devra faire un handoff vers un dernier agent sp√©cialis√© dans les recommandations. Il donnera au quatri√®me agent son r√©sum√© ainsi que les commentaires qu‚Äôil a mis en m√©moire.\n",
    "\n",
    "Le quatri√®me agent est cr√©atif, intelligent, et cherche √† r√©soudre les probl√®mes de la mani√®re la plus simple et la plus efficace possible. Il proposera, pour chaque probl√®me rencontr√© dans le rapport pr√©c√©dent, 3 projets/id√©es pour les r√©soudre. \n",
    "Important: les id√©es/r√©solutions de probl√®mes doivent √™tre le plus concret et simple possible\n",
    "avec des id√©es de SAAS et produits num√©riques en priorit√©s.\n",
    "Enfin, il retournera le rapport du troisi√®me agent ainsi que ses recommandations.\n",
    "\n",
    "Comme outils, il proposera une exportation en PDF, CSV ou TXT de son rapport, ainsi que la possibilit√© :\n",
    "\n",
    "soit de lister les commentaires marquants qu‚Äôil a en m√©moire avec leur r√©f√©rence ;\n",
    "\n",
    "soit de les exporter √©galement en PDF, CSV ou TXT.\n",
    "</fin>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = \"\"\"Pour une structuration optimale, voici quelques suggestions d'am√©lioration :\n",
    "\n",
    "### Clarification des r√¥les et interactions\n",
    "\n",
    "1. **Agent 1 : Interaction Utilisateur**\n",
    "   - **T√¢ches** : Politesse, guider l'utilisateur, valider les crit√®res de scraping.\n",
    "   - **Am√©liorations** : \n",
    "     - Ajoutez une √©tape de confirmation finale avec l'utilisateur pour consolider les param√®tres avant le handoff.\n",
    "     - Incluez des exemples concrets pour chaque crit√®re (top, new, etc.) pour faciliter la compr√©hension.\n",
    "\n",
    "2. **Agent 2 : Scraping des Donn√©es**\n",
    "   - **T√¢ches** : Scraping selon les crit√®res √©tablis.\n",
    "   - **Am√©liorations** : \n",
    "     - Assurez la compatibilit√© multilingue du scraper pour garantir une collecte de donn√©es sans faille.\n",
    "\n",
    "3. **Agent 3 : Analyse et Synth√®se**\n",
    "   - **T√¢ches** : Sentiment analysis, identification des probl√®mes.\n",
    "   - **Am√©liorations** : \n",
    "     - Impl√©mentez un syst√®me de filtre pour √©liminer les faux positifs dans l'analyse sentimentale.\n",
    "     - Pr√©cisez comment les scores de priorit√© sont calcul√©s.\n",
    "\n",
    "4. **Agent 4 : Recommandations**\n",
    "   - **T√¢ches** : Proposer des solutions concr√®tes.\n",
    "   - **Am√©liorations** : \n",
    "     - Validez la faisabilit√© des suggestions avec des exemples ou √©tudes de cas.\n",
    "     - Encouragez l'utilisation de frameworks existants pour les id√©es de SAAS.\n",
    "\n",
    "### Optimisation des Outils\n",
    "\n",
    "1. **Agent 1 :**\n",
    "   - Int√©grez des outils de v√©rification linguistique pour une gestion efficace des fautes d‚Äôorthographe.\n",
    "\n",
    "2. **Agent 2 :**\n",
    "   - Choisissez des outils de scraping robustes assurant la conformit√© avec les normes d‚Äôutilisation des donn√©es de Reddit.\n",
    "\n",
    "3. **Agent 3 :**\n",
    "   - Utilisez des biblioth√®ques √©prouv√©es pour l'analyse de sentiment multilingue.\n",
    "\n",
    "4. **Agent 4 :**\n",
    "   - Optez pour des biblioth√®ques d‚ÄôIA g√©n√©ratives pour formuler des solutions innovantes et pratiques.\n",
    "\n",
    "### Coordination et Gestion de Projet\n",
    "\n",
    "- Assurez une communication fluide entre les agents en utilisant des API standardis√©es et un format de donn√©es commun (JSON, XML).\n",
    "- D√©finissez des indicateurs de performance cl√© (KPI) pour √©valuer l'efficacit√© de chaque agent.\n",
    "- Pr√©voyez un m√©canisme de feedback pour am√©liorer continuellement le syst√®me en fonction des interactions utilisateur.\n",
    "\n",
    "En int√©grant ces am√©liorations, vous renforcerez la robustesse et l'efficacit√© de votre projet tout en optimisant l'exp√©rience utilisateur.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b89f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tu m'as donn√© les am√©liorations. Maintenant fait un TOUT COMPLET pour que je puisse le donner √† claude \\\n",
    "    Il doit tout y avoir, les prompts des agents, les outils tout !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef801052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour structurer ce projet de mani√®re optimale, voici mon retour :\n",
      "\n",
      "### Structure Optimis√©e des Agents et Interactions\n",
      "\n",
      "#### Agent 1 : Interaction Utilisateur\n",
      "- **T√¢ches** :\n",
      "  - Saluer l'utilisateur et expliquer le processus.\n",
      "  - Clarifier les crit√®res (top, new, best, rising, hot) et l'architecture Reddit.\n",
      "  - Confirmer les param√®tres : subreddit, nombre de posts/commentaires et crit√®res.\n",
      "- **Outils** :\n",
      "  - Recherche Internet pour v√©rifier et expliquer les subreddits.\n",
      "  - V√©rificateur d'existence de subreddit avec correction automatique.\n",
      "- **Prompts** :\n",
      "  - \"Bonjour ! Je vais vous aider √† identifier des points de douleur sur Reddit. Pouvez-vous me dire quel subreddit vous int√©resse et vos pr√©f√©rences de recherche ?\"\n",
      "  - \"Voici nos param√®tres par d√©faut : 10 posts, 10 commentaires, crit√®re top. Avez-vous des changements √† apporter ?\"\n",
      "- **Am√©liorations** :\n",
      "  - Pr√©voir des exemples concrets lors de l‚Äôexplication des crit√®res.\n",
      "\n",
      "#### Agent 2 : Scraping des Donn√©es\n",
      "- **T√¢ches** :\n",
      "  - Scraper les donn√©es en fonction des crit√®res et param√®tres confirm√©s par l‚ÄôAgent 1.\n",
      "- **Outils** :\n",
      "  - Outil de scraping respectant les r√®gles de Reddit.\n",
      "- **Prompts** :\n",
      "  - \"Les donn√©es demand√©es ont √©t√© r√©cup√©r√©es. Transfert vers l‚Äôanalyse en cours.\"\n",
      "- **Am√©liorations** :\n",
      "  - Assurer la compatibilit√© multilingue et conforme aux politiques de donn√©es.\n",
      "\n",
      "#### Agent 3 : Analyse et Synth√®se\n",
      "- **T√¢ches** :\n",
      "  - Analyse de sentiment, identification des douleurs, calcul des scores de priorit√©.\n",
      "- **Outils** :\n",
      "  - Biblioth√®ques d‚Äôanalyse de sentiment.\n",
      "- **Prompts** :\n",
      "  - \"Analyse des sentiments en cours. Identification des principales douleurs des utilisateurs...\"\n",
      "  - \"R√©sum√© complet pr√™t. Voulez-vous voir les solutions sugg√©r√©es ?\"\n",
      "- **Am√©liorations** :\n",
      "  - Impl√©menter des filtres pour √©viter les faux positifs.\n",
      "\n",
      "#### Agent 4 : Recommandations\n",
      "- **T√¢ches** :\n",
      "  - Proposer des solutions concr√®tes et simplifi√©es pour chaque probl√®me identifi√©.\n",
      "- **Outils** :\n",
      "  - Biblioth√®ques d‚ÄôIA g√©n√©ratives pour recommandations.\n",
      "- **Prompts** :\n",
      "  - \"Je propose ces trois solutions pour r√©soudre le probl√®me identifi√©...\"\n",
      "- **Am√©liorations** :\n",
      "  - S'assurer de la faisabilit√© des solutions propos√©es avec exemples pratiques.\n",
      "\n",
      "### Coordination et Gestion du Projet\n",
      "\n",
      "- **Interconnexions** :\n",
      "  - Utiliser JSON pour le transfert de donn√©es standardis√© entre les agents.\n",
      "- **Indicateurs de Performance (KPI)** :\n",
      "  - Suivre le taux de satisfaction utilisateur, la pr√©cision des analyses, et la pertinence des recommandations.\n",
      "- **Feedback et Am√©lioration Continue** :\n",
      "  - Mettre en place un syst√®me de feedback utilisateur pour ajuster et am√©liorer les r√©ponses des agents.\n",
      "\n",
      "### Formats de Sortie\n",
      "\n",
      "- **Exports Disponibles** :\n",
      "  - PDF, CSV, TXT\n",
      "- **Options** :\n",
      "  - Options pour afficher ou exporter les commentaires marquants.\n",
      "\n",
      "Int√©grer ces aspects assurera une meilleure fluidit√©, efficacit√©, et satisfaction dans la gestion de projet.\n"
     ]
    }
   ],
   "source": [
    "with trace(\"Structure\"):\n",
    "    result = await Runner.run(agent,question + history + message)\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with trace(\"test\"):\n",
    "    result = await Runner.run(agent, \"Write a haiku about recursion in programming\")\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Tu es un assistant sp√©cialis√© pour aider les entrepreneurs √† trouver des probl√®mes √† r√©soudre sur\n",
    " Reddit.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Demander le subreddit √† analyser\n",
    "2. Proposer les options de tri disponibles:\n",
    "   - ÔøΩÔøΩ \"Top\" ‚Üí posts avec le meilleur score (upvotes - downvotes) sur une p√©riode\n",
    "   - üÜï \"New\" ‚Üí les plus r√©cents\n",
    "   - üí¨ \"Hot\" ‚Üí combinaison de score + fra√Æcheur (post r√©cent avec beaucoup d'activit√©)\n",
    "   - üåü \"Best\" ‚Üí pertinence + vote + r√©ponse (dans les commentaires)\n",
    "3. Demander combien de posts analyser\n",
    "4. Utiliser tes outils pour v√©rifier l'existence du subreddit et rechercher des informations si n√©cessaire\n",
    "\n",
    "Tu peux utiliser comme outils:\n",
    "- check_subreddit_exists: pour v√©rifier si un subreddit existe\n",
    "- WebSearchTool: pour rechercher des informations r√©centes sur les subreddits ou probl√®mes entrepreneurs\n",
    "\n",
    "Sois toujours poli, professionnel et aide l'utilisateur √† faire le meilleur choix pour ses besoins.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outil pour v√©rifier si un subreddit existe\n",
    "@function_tool\n",
    "def check_subreddit_exists(subreddit_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    V√©rifie si un subreddit existe en faisant une requ√™te √† Reddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://www.reddit.com/r/{subreddit_name}/about.json\"\n",
    "        headers = {\n",
    "            'User-Agent': 'RedditScraper/1.0'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        print(response.status_code)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'data' in data and 'subscribers' in data['data']:\n",
    "                return {\n",
    "                    \"exists\": True,\n",
    "                    \"subscribers\": data['data']['subscribers'],\n",
    "                    \"description\": data['data'].get('public_description', ''),\n",
    "                    \"title\": data['data'].get('title', '')\n",
    "                }\n",
    "        \n",
    "        return {\"exists\": False, \"error\": \"Subreddit non trouv√©\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"exists\": False, \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dab60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de l'outil\n",
    "print(\"Test de v√©rification de subreddit:\")\n",
    "test_subreddits = [\"france\", \"programming\", \"nonexistentsubreddit123\"]\n",
    "for sub in test_subreddits:\n",
    "    result = check_subreddit_exists(sub)\n",
    "    print(f\"r/{sub}: {'‚úÖ Existe' if result['exists'] else '‚ùå N\\'existe pas'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb64de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07074805",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"prix actions tesla aujourdhui, utilise recherche web\"\n",
    "\n",
    "with trace(\"test\"):\n",
    "    result = await Runner.run(agent, message)\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "\n",
    "    response = Runner.run(agent, message)\n",
    "    reply =response.choices[0].message.content      \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a38fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(message, history):\n",
    "    \"\"\"\n",
    "    Fonction de chat asynchrone pour Gradio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with trace(\"chat_interaction\"):\n",
    "            result = await Runner.run(agent, message)\n",
    "            return result.final_output\n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {str(e)}\"\n",
    "\n",
    "# Interface Gradio avec fonction asynchrone\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fed059",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_history(message, history):\n",
    "    \"\"\"\n",
    "    Fonction de chat avec historique int√©gr√© de Gradio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construire le contexte avec l'historique\n",
    "        full_context = \"\"\n",
    "        for human, assistant in history:\n",
    "            full_context += f\"Humain: {human}\\nAssistant: {assistant}\\n\"\n",
    "        \n",
    "        # Ajouter le message actuel\n",
    "        full_context += f\"Humain: {message}\\nAssistant: \"\n",
    "        \n",
    "        with trace(\"chat_with_history\"):\n",
    "            result = await Runner.run(agent, full_context)\n",
    "            return result.final_output\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Erreur: {str(e)}\"\n",
    "\n",
    "# Interface avec historique automatique\n",
    "gr.ChatInterface(\n",
    "    chat_with_history, \n",
    "    title=\"Assistant Reddit\",\n",
    "    description=\"Posez vos questions sur Reddit et les entrepreneurs\",\n",
    "    examples=[\n",
    "        \"V√©rifie si le subreddit 'entrepreneur' existe\",\n",
    "        \"Quels sont les probl√®mes courants des entrepreneurs ?\",\n",
    "        \"Recherche des informations sur les startups en France\"\n",
    "    ]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Reddit (utilise les variables d√©j√† d√©finies)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def scrape_subreddit_top_posts(subreddit_name: str, num_posts: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts avec le meilleur score d'un subreddit donn√©\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit (sans le 'r/')\n",
    "        num_posts: Nombre de posts √† r√©cup√©rer (d√©faut: 10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict contenant les posts scrap√©s avec leurs d√©tails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # V√©rifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # R√©cup√©rer les posts avec le meilleur score\n",
    "        posts_data = []\n",
    "        \n",
    "        for post in subreddit.top(limit=num_posts, time_filter='month'):\n",
    "            # R√©cup√©rer les commentaires principaux\n",
    "            post.comments.replace_more(limit=5)  # Limiter les commentaires imbriqu√©s\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:10]:  # Top 10 commentaires\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:500] + \"...\" if len(post.selftext) > 500 else post.selftext,\n",
    "                \"comments\": comments_data\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name\n",
    "        }\n",
    "\n",
    "# Test de la fonction\n",
    "print(\"üß™ Test du scraping Reddit:\")\n",
    "test_result = scrape_subreddit_top_posts(\"programming\", 3)\n",
    "print(f\"‚úÖ Posts r√©cup√©r√©s: {test_result['posts_count'] if test_result['success'] else '‚ùå Erreur'}\")\n",
    "if test_result['success']:\n",
    "    for i, post in enumerate(test_result['posts'], 1):\n",
    "        print(f\"  {i}. {post['title'][:50]}... (Score: {post['score']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def scrape_subreddit_posts(subreddit_name: str, num_posts: int = 10, sort_criteria: str = \"top\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts d'un subreddit selon diff√©rents crit√®res de tri\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit (sans le 'r/')\n",
    "        num_posts: Nombre de posts √† r√©cup√©rer (d√©faut: 10)\n",
    "        sort_criteria: Crit√®re de tri - \"top\", \"new\", \"hot\", \"best\", \"rising\" (d√©faut: \"top\")\n",
    "    \n",
    "    Returns:\n",
    "        Dict contenant les posts scrap√©s avec leurs d√©tails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # V√©rifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Mapper les crit√®res de tri aux m√©thodes PRAW\n",
    "        sort_methods = {\n",
    "            \"top\": subreddit.top,\n",
    "            \"new\": subreddit.new,\n",
    "            \"hot\": subreddit.hot,\n",
    "            \"best\": subreddit.best,\n",
    "            \"rising\": subreddit.rising\n",
    "        }\n",
    "        \n",
    "        # V√©rifier que le crit√®re est valide\n",
    "        if sort_criteria not in sort_methods:\n",
    "            sort_criteria = \"top\"  # D√©faut si crit√®re invalide\n",
    "        \n",
    "        # R√©cup√©rer les posts selon le crit√®re choisi\n",
    "        posts_data = []\n",
    "        \n",
    "        # Appliquer le tri avec time_filter pour \"top\" et \"rising\"\n",
    "        if sort_criteria in [\"top\", \"rising\"]:\n",
    "            posts = sort_methods[sort_criteria](limit=num_posts, time_filter='month')\n",
    "        else:\n",
    "            posts = sort_methods[sort_criteria](limit=num_posts)\n",
    "        \n",
    "        for post in posts:\n",
    "            # R√©cup√©rer les commentaires principaux\n",
    "            post.comments.replace_more(limit=5)\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:10]:\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:500] + \"...\" if len(post.selftext) > 500 else post.selftext,\n",
    "                \"comments\": comments_data\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91560332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "\n",
    "@function_tool\n",
    "def export_analysis_to_txt(analysis_text: str, filename: str = \"reddit_analysis.txt\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte l'analyse Reddit au format TXT\n",
    "    \n",
    "    Args:\n",
    "        analysis_text: Texte de l'analyse √† exporter\n",
    "        filename: Nom du fichier (d√©faut: reddit_analysis.txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cr√©er le dossier exports s'il n'existe pas\n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        filepath = export_dir / filename\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"ANALYSE REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(analysis_text)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"filepath\": str(filepath),\n",
    "            \"message\": f\"‚úÖ Analyse export√©e en TXT: {filepath}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def export_analysis_to_csv(analysis_data: Dict, filename: str = \"reddit_analysis.csv\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte l'analyse Reddit au format CSV\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Donn√©es structur√©es de l'analyse\n",
    "        filename: Nom du fichier (d√©faut: reddit_analysis.csv)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        filepath = export_dir / filename\n",
    "        \n",
    "        with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Section', 'Contenu'])\n",
    "            \n",
    "            # Structurer les donn√©es pour CSV\n",
    "            sections = analysis_data.get('sections', {})\n",
    "            for section, content in sections.items():\n",
    "                writer.writerow([section, content])\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"filepath\": str(filepath),\n",
    "            \"message\": f\"‚úÖ Analyse export√©e en CSV: {filepath}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def export_analysis_to_pdf(analysis_text: str, filename: str = \"reddit_analysis.pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte l'analyse Reddit au format PDF\n",
    "    \n",
    "    Args:\n",
    "        analysis_text: Texte de l'analyse √† exporter\n",
    "        filename: Nom du fichier (d√©faut: reddit_analysis.pdf)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        filepath = export_dir / filename\n",
    "        \n",
    "        doc = SimpleDocTemplate(str(filepath), pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        story = []\n",
    "        \n",
    "        # Titre\n",
    "        title = Paragraph(f\"ANALYSE REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", styles['Title'])\n",
    "        story.append(title)\n",
    "        story.append(Spacer(1, 12))\n",
    "        \n",
    "        # Contenu\n",
    "        content = Paragraph(analysis_text.replace('\\n', '<br/>'), styles['Normal'])\n",
    "        story.append(content)\n",
    "        \n",
    "        doc.build(story)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"filepath\": str(filepath),\n",
    "            \"message\": f\"‚úÖ Analyse export√©e en PDF: {filepath}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43135a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'agent sp√©cialis√© pour l'analyse Reddit\n",
    "reddit_analyzer_agent = Agent(\n",
    "    name=\"RedditAnalyzer\",\n",
    "    instructions=\"\"\"Tu es un expert analyste sp√©cialis√© dans l'analyse de contenu Reddit.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Scraper les posts d'un subreddit sp√©cifi√©\n",
    "2. Analyser le contenu des posts et commentaires\n",
    "3. Fournir un r√©sum√© structur√© et des insights\n",
    "\n",
    "Instructions sp√©cifiques:\n",
    "- Demande toujours le nom du subreddit et le nombre de posts √† analyser\n",
    "- Utilise l'outil scrape_subreddit_top_posts pour r√©cup√©rer les donn√©es\n",
    "- Analyse les th√®mes r√©currents, probl√®mes mentionn√©s, sentiments\n",
    "- Fournis un r√©sum√© en fran√ßais avec:\n",
    "  * Th√®mes principaux identifi√©s\n",
    "  * Probl√®mes r√©currents\n",
    "  * Insights int√©ressants\n",
    "  * Recommandations pour les entrepreneurs\n",
    "\n",
    "Sois toujours professionnel et fournis des analyses utiles.\"\"\",\n",
    "    tools=[\n",
    "        scrape_subreddit_top_posts,\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fonction pour lancer l'analyse\n",
    "async def analyze_subreddit(subreddit_name: str, num_posts: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Analyse compl√®te d'un subreddit avec r√©sum√©\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit\n",
    "        num_posts: Nombre de posts √† analyser\n",
    "    \n",
    "    Returns:\n",
    "        R√©sum√© d'analyse structur√©\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construire le prompt d'analyse\n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyse le subreddit r/{subreddit_name} en r√©cup√©rant les {num_posts} posts avec le meilleur score.\n",
    "        \n",
    "        Fournis un r√©sum√© structur√© incluant:\n",
    "        1. üìä Vue d'ensemble du subreddit\n",
    "        2. üéØ Th√®mes principaux identifi√©s\n",
    "        3. ‚ö†Ô∏è Probl√®mes r√©currents mentionn√©s\n",
    "        4. üí° Insights int√©ressants pour les entrepreneurs\n",
    "        5. ÔøΩÔøΩ Opportunit√©s identifi√©es\n",
    "        6. üìà Tendances observ√©es\n",
    "        \n",
    "        Sois pr√©cis et utilise les donn√©es r√©elles scrap√©es.\n",
    "        \"\"\"\n",
    "        \n",
    "        with trace(\"reddit_analysis\"):\n",
    "            result = await Runner.run(reddit_analyzer_agent, analysis_prompt)\n",
    "            return result.final_output\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erreur lors de l'analyse: {str(e)}\"\n",
    "\n",
    "# Test de l'analyse\n",
    "print(\"üîç Test de l'analyse Reddit:\")\n",
    "test_analysis = await analyze_subreddit(\"entrepreneur\", 5)\n",
    "print(\"=\" * 50)\n",
    "print(\"R√âSULTAT DE L'ANALYSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(test_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82559156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface Gradio pour l'analyse Reddit - Version corrig√©e\n",
    "async def reddit_analysis_interface(message, history):\n",
    "    \"\"\"\n",
    "    Interface pour analyser un subreddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extraire les param√®tres du message\n",
    "        # Format attendu: \"subreddit: nom_du_subreddit, posts: nombre\"\n",
    "        if \":\" in message:\n",
    "            parts = message.split(\",\")\n",
    "            subreddit_name = \"entrepreneur\"  # d√©faut\n",
    "            num_posts = 10  # d√©faut\n",
    "            \n",
    "            for part in parts:\n",
    "                if \"subreddit:\" in part.lower():\n",
    "                    subreddit_name = part.split(\":\")[1].strip()\n",
    "                elif \"posts:\" in part.lower():\n",
    "                    try:\n",
    "                        num_posts = int(part.split(\":\")[1].strip())\n",
    "                    except:\n",
    "                        num_posts = 10\n",
    "        else:\n",
    "            # Si pas de format sp√©cifique, utiliser le message comme nom de subreddit\n",
    "            subreddit_name = message.strip()\n",
    "            num_posts = 10\n",
    "        \n",
    "        # Construire le prompt d'analyse\n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyse le subreddit r/{subreddit_name} en r√©cup√©rant les {num_posts} posts avec le meilleur score.\n",
    "        \n",
    "        Fournis un r√©sum√© structur√© incluant:\n",
    "        1. üìä Vue d'ensemble du subreddit\n",
    "        2. üéØ Th√®mes principaux identifi√©s\n",
    "        3. ‚ö†Ô∏è Probl√®mes r√©currents mentionn√©s\n",
    "        4. üí° Insights int√©ressants pour les entrepreneurs\n",
    "        5. ÔøΩÔøΩ Opportunit√©s identifi√©es\n",
    "        6. üìà Tendances observ√©es\n",
    "        \n",
    "        Sois pr√©cis et utilise les donn√©es r√©elles scrap√©es.\n",
    "        \"\"\"\n",
    "        \n",
    "        with trace(\"gradio_analysis\"):\n",
    "            result = await Runner.run(reddit_analyzer_agent, analysis_prompt)\n",
    "            return result.final_output\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erreur: {str(e)}\"\n",
    "\n",
    "# Interface Gradio corrig√©e\n",
    "with gr.Blocks(title=\"Analyseur Reddit\") as demo:\n",
    "    gr.Markdown(\"# üîç Analyseur de Subreddits\")\n",
    "    gr.Markdown(\"Analysez les posts populaires d'un subreddit pour identifier des opportunit√©s entrepreneuriales\")\n",
    "    \n",
    "    # Interface de chat avec format messages\n",
    "    chatbot = gr.ChatInterface(\n",
    "        reddit_analysis_interface,\n",
    "        title=\"Analyse Reddit\",\n",
    "        description=\"Tapez le nom d'un subreddit ou utilisez le format: 'subreddit: nom, posts: nombre'\",\n",
    "        examples=[\n",
    "            \"entrepreneur\",\n",
    "            \"startup\",\n",
    "            \"smallbusiness\",\n",
    "            \"subreddit: programming, posts: 15\",\n",
    "            \"subreddit: freelance, posts: 8\"\n",
    "        ],\n",
    "        type=\"messages\"  # Format moderne\n",
    "    )\n",
    "    \n",
    "    # Instructions d'utilisation\n",
    "    gr.Markdown(\"### üìù Instructions d'utilisation:\")\n",
    "    gr.Markdown(\"- **Simple** : Tapez juste le nom du subreddit (ex: 'entrepreneur')\")\n",
    "    gr.Markdown(\"- **Avanc√©** : Utilisez le format 'subreddit: nom, posts: nombre'\")\n",
    "    gr.Markdown(\"- **Exemples** : Cliquez sur les exemples ci-dessus\")\n",
    "    \n",
    "    gr.Markdown(\"### üéØ Subreddits populaires √† analyser:\")\n",
    "    gr.Markdown(\"- **entrepreneur** - Probl√®mes d'entrepreneurs\")\n",
    "    gr.Markdown(\"- **startup** - D√©fis des startups\")\n",
    "    gr.Markdown(\"- **smallbusiness** - Petites entreprises\")\n",
    "    gr.Markdown(\"- **freelance** - Freelancing et ind√©pendants\")\n",
    "    gr.Markdown(\"- **programming** - Probl√®mes techniques\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version alternative avec inputs s√©par√©s\n",
    "async def analyze_with_inputs(subreddit_name: str, num_posts: int):\n",
    "    \"\"\"\n",
    "    Analyse avec param√®tres s√©par√©s\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyse le subreddit r/{subreddit_name} en r√©cup√©rant les {num_posts} posts avec le meilleur score.\n",
    "        \n",
    "        Fournis un r√©sum√© structur√© incluant:\n",
    "        1. üìä Vue d'ensemble du subreddit\n",
    "        2. üéØ Th√®mes principaux identifi√©s\n",
    "        3. ‚ö†Ô∏è Probl√®mes r√©currents mentionn√©s\n",
    "        4. üí° Insights int√©ressants pour les entrepreneurs\n",
    "        5. ÔøΩÔøΩ Opportunit√©s identifi√©es\n",
    "        6. üìà Tendances observ√©es\n",
    "        \"\"\"\n",
    "        \n",
    "        with trace(\"analysis_with_inputs\"):\n",
    "            result = await Runner.run(reddit_analyzer_agent, analysis_prompt)\n",
    "            return result.final_output\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erreur: {str(e)}\"\n",
    "\n",
    "# Interface alternative\n",
    "with gr.Blocks(title=\"Analyseur Reddit - Version Simple\") as demo:\n",
    "    gr.Markdown(\"# üîç Analyseur de Subreddits\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            subreddit_input = gr.Textbox(\n",
    "                label=\"Nom du subreddit\",\n",
    "                placeholder=\"ex: entrepreneur\",\n",
    "                value=\"entrepreneur\"\n",
    "            )\n",
    "            posts_slider = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=20,\n",
    "                value=10,\n",
    "                step=1,\n",
    "                label=\"Nombre de posts\"\n",
    "            )\n",
    "            analyze_btn = gr.Button(\"ÔøΩÔøΩ Analyser\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_text = gr.Textbox(\n",
    "                label=\"R√©sultat de l'analyse\",\n",
    "                lines=20,\n",
    "                interactive=False\n",
    "            )\n",
    "    \n",
    "    # Connecter le bouton\n",
    "    analyze_btn.click(\n",
    "        analyze_with_inputs,\n",
    "        inputs=[subreddit_input, posts_slider],\n",
    "        outputs=output_text\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def analyze_user_pains_from_posts(posts_data: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Outil pour analyser les douleurs utilisateurs √† partir de posts Reddit\n",
    "    \n",
    "    Args:\n",
    "        posts_data: Liste des posts scrap√©s par reddit_analyzer_agent\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les douleurs identifi√©es, scores et r√©sum√©\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pain_patterns = {\n",
    "            \"probl√®me\": [\"probl√®me\", \"difficile\", \"compliqu√©\", \"emb√™tant\"],\n",
    "            \"frustration\": [\"frustr√©\", \"√©nerv√©\", \"agac√©\", \"exasp√©r√©\"],\n",
    "            \"difficult√©\": [\"gal√®re\", \"casse-t√™te\", \"obstacle\", \"blocage\"],\n",
    "            \"stress\": [\"stress√©\", \"inquiet\", \"anxieux\", \"paniqu√©\"],\n",
    "            \"confusion\": [\"perdu\", \"confus\", \"pas clair\", \"incompr√©hensible\"],\n",
    "            \"co√ªt\": [\"cher\", \"co√ªteux\", \"budget\", \"prix\"],\n",
    "            \"temps\": [\"long\", \"lent\", \"chronophage\", \"perte de temps\"],\n",
    "            \"technique\": [\"bug\", \"erreur\", \"plantage\", \"dysfonctionnement\"]\n",
    "        }\n",
    "        \n",
    "        pain_analysis = {}\n",
    "        \n",
    "        for post in posts_data:\n",
    "            text_to_analyze = f\"{post['title']} {post['selftext']}\".lower()\n",
    "            \n",
    "            for pain_type, keywords in pain_patterns.items():\n",
    "                count = sum(1 for keyword in keywords if keyword in text_to_analyze)\n",
    "                if count > 0:\n",
    "                    if pain_type not in pain_analysis:\n",
    "                        pain_analysis[pain_type] = {\n",
    "                            \"posts\": [],\n",
    "                            \"total_upvotes\": 0,\n",
    "                            \"total_comments\": 0,\n",
    "                            \"frequency\": 0\n",
    "                        }\n",
    "                    \n",
    "                    pain_analysis[pain_type][\"posts\"].append(post)\n",
    "                    pain_analysis[pain_type][\"total_upvotes\"] += post[\"score\"]\n",
    "                    pain_analysis[pain_type][\"total_comments\"] += post[\"num_comments\"]\n",
    "                    pain_analysis[pain_type][\"frequency\"] += 1\n",
    "        \n",
    "        # Calculer les scores\n",
    "        pain_scores = []\n",
    "        for pain_type, data in pain_analysis.items():\n",
    "            if data[\"frequency\"] > 0:\n",
    "                avg_upvotes = data[\"total_upvotes\"] / data[\"frequency\"]\n",
    "                avg_comments = data[\"total_comments\"] / data[\"frequency\"]\n",
    "                score = (data[\"frequency\"] * 0.6) + (avg_upvotes * 0.3) + (avg_comments * 0.1)\n",
    "                \n",
    "                pain_scores.append({\n",
    "                    \"pain_type\": pain_type,\n",
    "                    \"frequency\": data[\"frequency\"],\n",
    "                    \"avg_upvotes\": round(avg_upvotes, 2),\n",
    "                    \"avg_comments\": round(avg_comments, 2),\n",
    "                    \"score\": round(score, 2),\n",
    "                    \"posts_count\": len(data[\"posts\"])\n",
    "                })\n",
    "        \n",
    "        pain_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"total_posts_analyzed\": len(posts_data),\n",
    "            \"pains_identified\": len(pain_scores),\n",
    "            \"top_pains\": pain_scores[:5],\n",
    "            \"all_pains\": pain_scores\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Agent mis √† jour avec l'outil d'analyse des douleurs\n",
    "reddit_analyzer_agent = Agent(\n",
    "    name=\"RedditAnalyzer\",\n",
    "    instructions=\"\"\"Tu es un expert analyste sp√©cialis√© dans l'analyse de contenu Reddit.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Scraper les posts d'un subreddit sp√©cifi√©\n",
    "2. Analyser le contenu des posts et commentaires\n",
    "3. Identifier les douleurs utilisateurs r√©currentes\n",
    "4. Fournir un r√©sum√© structur√© et des insights\n",
    "\n",
    "Instructions sp√©cifiques:\n",
    "- Demande toujours le nom du subreddit et le nombre de posts √† analyser\n",
    "- Utilise scrape_subreddit_posts pour r√©cup√©rer les donn√©es\n",
    "- Utilise analyze_user_pains_from_posts pour identifier les douleurs\n",
    "- Fournis un r√©sum√© en fran√ßais avec:\n",
    "  * Vue d'ensemble du subreddit\n",
    "  * Top 5 douleurs utilisateurs identifi√©es\n",
    "  * Scores de priorit√© pour chaque douleur\n",
    "  * Insights int√©ressants pour les entrepreneurs\n",
    "  * Opportunit√©s identifi√©es\n",
    "- √Ä la fin, propose d'exporter en TXT, CSV ou PDF\n",
    "\n",
    "M√©thodologie de scoring des douleurs:\n",
    "- Fr√©quence = nombre de posts mentionnant cette douleur\n",
    "- Int√©r√™t = moyenne des upvotes et commentaires\n",
    "- Score final = (fr√©quence * 0.6) + (moyenne_upvotes * 0.3) + (moyenne_commentaires * 0.1)\n",
    "\n",
    "Sois toujours professionnel et fournis des analyses utiles.\"\"\",\n",
    "    tools=[\n",
    "        scrape_subreddit_posts,\n",
    "        analyze_user_pains_from_posts,\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists,\n",
    "        export_analysis_to_txt,\n",
    "        export_analysis_to_csv,\n",
    "        export_analysis_to_pdf\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent sp√©cialis√© dans l'identification des douleurs utilisateurs\n",
    "pain_identifier_agent = Agent(\n",
    "    name=\"PainIdentifier\",\n",
    "    instructions=\"\"\"Tu es un expert analyste sp√©cialis√© dans l'identification des probl√®mes et frustrations r√©currents des utilisateurs.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Analyser le sentiment et l'intensit√© √©motionnelle de chaque post/commentaire\n",
    "2. Identifier les douleurs r√©currentes ind√©pendamment de la langue\n",
    "3. Lister les 5 principales douleurs en une phrase chacune\n",
    "4. Calculer un score de priorit√© pour chaque douleur\n",
    "5. Classer les douleurs par ordre de priorit√©\n",
    "\n",
    "Instructions sp√©cifiques:\n",
    "- Analyse le SENTIMENT (frustration, col√®re, stress, confusion, etc.)\n",
    "- √âvalue l'INTENSIT√â √©motionnelle (1-10, 10 = tr√®s intense)\n",
    "- Identifie le TYPE de probl√®me (technique, financier, relationnel, etc.)\n",
    "- Regroupe les douleurs similaires\n",
    "- Calcule les m√©triques pour chaque douleur\n",
    "- Pr√©sente les top 3 douleurs avec leurs scores\n",
    "\n",
    "Format de sortie attendu:\n",
    "1. Liste des 5 principales douleurs (une phrase chacune)\n",
    "2. Top 3 class√© par score\n",
    "3. R√©sum√© en un paragraphe\n",
    "4. Recommandations pour les entrepreneurs\n",
    "\n",
    "Sois pr√©cis dans l'identification √©motionnelle et le scoring des probl√®mes.\"\"\",\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def identify_user_pains_llm(posts_data: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Outil pour identifier les douleurs utilisateurs via analyse LLM\n",
    "    \n",
    "    Args:\n",
    "        posts_data: Liste des posts scrap√©s\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les douleurs identifi√©es, scores et r√©sum√©\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pr√©parer les donn√©es pour l'analyse LLM\n",
    "        posts_for_analysis = []\n",
    "        \n",
    "        for post in posts_data:\n",
    "            # Combiner titre, contenu et commentaires\n",
    "            full_text = f\"Titre: {post['title']}\\n\"\n",
    "            if post.get('selftext'):\n",
    "                full_text += f\"Contenu: {post['selftext']}\\n\"\n",
    "            \n",
    "            # Ajouter les commentaires principaux\n",
    "            if post.get('comments'):\n",
    "                full_text += \"Commentaires:\\n\"\n",
    "                for comment in post['comments'][:5]:  # Top 5 commentaires\n",
    "                    full_text += f\"- {comment.get('body', '')}\\n\"\n",
    "            \n",
    "            posts_for_analysis.append({\n",
    "                \"text\": full_text,\n",
    "                \"score\": post[\"score\"],\n",
    "                \"num_comments\": post[\"num_comments\"],\n",
    "                \"url\": post[\"url\"]\n",
    "            })\n",
    "        \n",
    "        # Analyser chaque post avec le LLM\n",
    "        pain_analysis = {}\n",
    "        \n",
    "        for i, post in enumerate(posts_for_analysis):\n",
    "            analysis_prompt = f\"\"\"\n",
    "            Analyse ce post Reddit pour identifier les douleurs utilisateurs.\n",
    "            \n",
    "            POST #{i+1}:\n",
    "            {post['text']}\n",
    "            \n",
    "            Analyse ce post et r√©ponds au format JSON:\n",
    "            {{\n",
    "                \"sentiment\": \"frustration/col√®re/stress/confusion/autre\",\n",
    "                \"intensity\": 1-10,\n",
    "                \"pain_types\": [\"type1\", \"type2\"],\n",
    "                \"description\": \"description courte de la douleur\"\n",
    "            }}\n",
    "            \n",
    "            R√®gles:\n",
    "            - sentiment: type d'√©motion principale\n",
    "            - intensity: 1-10 (10 = tr√®s intense)\n",
    "            - pain_types: types de probl√®mes (technique, financier, relationnel, temps, etc.)\n",
    "            - description: r√©sum√© en une phrase\n",
    "            \"\"\"\n",
    "            \n",
    "            with trace(f\"post_analysis_{i}\"):\n",
    "                result = await Runner.run(pain_analyzer_agent, analysis_prompt)\n",
    "                \n",
    "                try:\n",
    "                    # Parser la r√©ponse JSON\n",
    "                    analysis = json.loads(result.final_output)\n",
    "                    \n",
    "                    # Traiter chaque type de douleur identifi√©\n",
    "                    for pain_type in analysis.get('pain_types', []):\n",
    "                        if pain_type not in pain_analysis:\n",
    "                            pain_analysis[pain_type] = {\n",
    "                                \"posts\": [],\n",
    "                                \"total_upvotes\": 0,\n",
    "                                \"total_comments\": 0,\n",
    "                                \"frequency\": 0,\n",
    "                                \"total_intensity\": 0,\n",
    "                                \"descriptions\": []\n",
    "                            }\n",
    "                        \n",
    "                        pain_analysis[pain_type][\"posts\"].append(post)\n",
    "                        pain_analysis[pain_type][\"total_upvotes\"] += post[\"score\"]\n",
    "                        pain_analysis[pain_type][\"total_comments\"] += post[\"num_comments\"]\n",
    "                        pain_analysis[pain_type][\"frequency\"] += 1\n",
    "                        pain_analysis[pain_type][\"total_intensity\"] += analysis.get('intensity', 5)\n",
    "                        pain_analysis[pain_type][\"descriptions\"].append(analysis.get('description', ''))\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    # Si le LLM ne retourne pas du JSON valide, continuer\n",
    "                    continue\n",
    "        \n",
    "        # Calculer les scores pour chaque douleur\n",
    "        pain_scores = []\n",
    "        for pain_type, data in pain_analysis.items():\n",
    "            if data[\"frequency\"] > 0:\n",
    "                avg_upvotes = data[\"total_upvotes\"] / data[\"frequency\"]\n",
    "                avg_comments = data[\"total_comments\"] / data[\"frequency\"]\n",
    "                avg_intensity = data[\"total_intensity\"] / data[\"frequency\"]\n",
    "                \n",
    "                # Score pond√©r√© incluant l'intensit√© √©motionnelle\n",
    "                score = (data[\"frequency\"] * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (avg_intensity * 0.3)\n",
    "                \n",
    "                pain_scores.append({\n",
    "                    \"pain_type\": pain_type,\n",
    "                    \"frequency\": data[\"frequency\"],\n",
    "                    \"avg_upvotes\": round(avg_upvotes, 2),\n",
    "                    \"avg_comments\": round(avg_comments, 2),\n",
    "                    \"avg_intensity\": round(avg_intensity, 2),\n",
    "                    \"score\": round(score, 2),\n",
    "                    \"posts_count\": len(data[\"posts\"]),\n",
    "                    \"descriptions\": data[\"descriptions\"][:3]  # Top 3 descriptions\n",
    "                })\n",
    "        \n",
    "        pain_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        # R√©sum√© final avec le LLM\n",
    "        summary_prompt = f\"\"\"\n",
    "        Analyse les douleurs utilisateurs identifi√©es dans les posts Reddit.\n",
    "        \n",
    "        Donn√©es d'analyse:\n",
    "        - Posts analys√©s: {len(posts_data)}\n",
    "        - Douleurs identifi√©es: {len(pain_scores)}\n",
    "        \n",
    "        Top 5 douleurs par score:\n",
    "        {json.dumps(pain_scores[:5], indent=2, ensure_ascii=False)}\n",
    "        \n",
    "        T√¢ches:\n",
    "        1. Liste les 5 principales douleurs en une phrase chacune\n",
    "        2. Pr√©sente les top 3 avec leurs scores d√©taill√©s\n",
    "        3. R√©sume les probl√®mes en un court paragraphe\n",
    "        4. Donne des recommandations pour les entrepreneurs\n",
    "        \n",
    "        Format de r√©ponse:\n",
    "        ## Top 5 Douleurs Identifi√©es\n",
    "        1. [Douleur 1] - Score: [X] (Intensit√©: [Y])\n",
    "        2. [Douleur 2] - Score: [X] (Intensit√©: [Y])\n",
    "        ...\n",
    "        \n",
    "        ## Top 3 Priorit√©s\n",
    "        1. [Douleur principale] - Score: [X] (Fr√©quence: [Y], Intensit√©: [Z])\n",
    "        2. [Deuxi√®me douleur] - Score: [X] (Fr√©quence: [Y], Intensit√©: [Z])\n",
    "        3. [Troisi√®me douleur] - Score: [X] (Fr√©quence: [Y], Intensit√©: [Z])\n",
    "        \n",
    "        ## R√©sum√©\n",
    "        [Paragraphe de r√©sum√©]\n",
    "        \n",
    "        ## Recommandations\n",
    "        [Recommandations pour les entrepreneurs]\n",
    "        \"\"\"\n",
    "        \n",
    "        with trace(\"final_analysis\"):\n",
    "            final_result = await Runner.run(pain_identifier_agent, summary_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"raw_data\": {\n",
    "                    \"total_posts_analyzed\": len(posts_data),\n",
    "                    \"pains_identified\": len(pain_scores),\n",
    "                    \"top_pains\": pain_scores[:5],\n",
    "                    \"all_pains\": pain_scores\n",
    "                },\n",
    "                \"analysis_summary\": final_result.final_output,\n",
    "                \"top_3_pains\": pain_scores[:3]\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test Kernel",
   "language": "python",
   "name": "test-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
