{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64df690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import sqlite3\n",
    "import praw\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, function_tool, trace, WebSearchTool\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a3af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = \"RedditScraper/1.0\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "Path(\"temp\").mkdir(exist_ok=True)\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ef1b8",
   "metadata": {},
   "source": [
    "# Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6257a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les param√®tres utilisateur\n",
    "USER_PARAMS_STRUCTURE = {\n",
    "    \"subreddit_name\": \"string\",  # Nom du subreddit\n",
    "    \"num_posts\": 10,             # Nombre de posts √† scraper (max 50)\n",
    "    \"comments_limit\": 10,        # Nombre de commentaires par post (max 50)\n",
    "    \"sort_criteria\": \"top\",      # top, new, hot, best, rising\n",
    "    \"time_filter\": \"month\",      # hour, day, week, month, year, all (pour top/rising)\n",
    "    \"confirmed\": True,           # Validation utilisateur\n",
    "    \"timestamp\": \"2024-01-01 12:00:00\",  # Date/heure de la demande\n",
    "    \"user_preferences\": {        # Pr√©f√©rences utilisateur\n",
    "        \"language\": \"auto\",      # Langue d'analyse\n",
    "        \"export_format\": \"all\"   # Format d'export pr√©f√©r√©\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les donn√©es scrap√©es\n",
    "SCRAPED_DATA_STRUCTURE = {\n",
    "    \"scraping_success\": True,           # Bool√©en\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"posts_count\": 10,                  # Nombre de posts r√©cup√©r√©s\n",
    "    \"total_comments\": 100,              # Total des commentaires\n",
    "    \"scraped_at\": \"2024-01-01 12:00:00\", # Date/heure du scraping\n",
    "    \"parameters_used\": {                # Param√®tres utilis√©s\n",
    "        \"sort_criteria\": \"top\",         # Crit√®re de tri\n",
    "        \"time_filter\": \"month\",         # Filtre temporel\n",
    "        \"comments_limit\": 10            # Limite de commentaires\n",
    "    },\n",
    "    \"posts\": [                          # Liste des posts\n",
    "        {\n",
    "            \"title\": \"string\",          # Titre du post\n",
    "            \"author\": \"string\",         # Auteur\n",
    "            \"score\": 100,               # Score (upvotes)\n",
    "            \"upvote_ratio\": 0.95,       # Ratio upvote\n",
    "            \"num_comments\": 50,         # Nombre de commentaires\n",
    "            \"created_utc\": \"2024-01-01 10:00:00\", # Date cr√©ation\n",
    "            \"url\": \"https://reddit.com/...\", # URL du post\n",
    "            \"selftext\": \"string\",       # Contenu du post\n",
    "            \"id\": \"string\",             # ID du post\n",
    "            \"comments\": [               # Liste des commentaires\n",
    "                {\n",
    "                    \"author\": \"string\", # Auteur du commentaire\n",
    "                    \"body\": \"string\",   # Contenu du commentaire\n",
    "                    \"score\": 10,        # Score du commentaire\n",
    "                    \"created_utc\": \"2024-01-01 10:30:00\", # Date cr√©ation\n",
    "                    \"id\": \"string\"      # ID du commentaire\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None               # Message d'erreur si √©chec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67583e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour l'analyse des douleurs\n",
    "PAIN_ANALYSIS_STRUCTURE = {\n",
    "    \"analysis_success\": True,           # Bool√©en\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit analys√©\n",
    "    \"posts_analyzed\": 10,               # Nombre de posts analys√©s\n",
    "    \"total_comments_analyzed\": 100,     # Total des commentaires analys√©s\n",
    "    \"analysis_timestamp\": \"2024-01-01 12:00:00\", # Date/heure de l'analyse\n",
    "    \"pain_categories\": [                # Liste des cat√©gories de douleurs\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur (ex: \"technique\", \"financier\")\n",
    "            \"frequency\": 5,             # Nombre de posts mentionnant cette douleur\n",
    "            \"avg_upvotes\": 25.5,        # Moyenne des upvotes\n",
    "            \"avg_comments\": 12.3,       # Moyenne des commentaires\n",
    "            \"avg_intensity\": 7.2,       # Intensit√© √©motionnelle moyenne (1-10)\n",
    "            \"score\": 45.6,              # Score de priorit√© calcul√©\n",
    "            \"rank\": 1,                  # Rang dans le classement\n",
    "            \"descriptions\": [           # Exemples de descriptions\n",
    "                \"Description 1\",\n",
    "                \"Description 2\"\n",
    "            ],\n",
    "            \"posts_affected\": [         # IDs des posts concern√©s\n",
    "                \"post_id_1\",\n",
    "                \"post_id_2\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_pains\": [                   # Top 3 des douleurs prioritaires\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"score\": 45.6,\n",
    "            \"frequency\": 5,\n",
    "            \"avg_intensity\": 7.2,\n",
    "            \"summary\": \"R√©sum√© de la douleur\"\n",
    "        }\n",
    "    ],\n",
    "    \"overall_summary\": \"string\",        # R√©sum√© global de l'analyse\n",
    "    \"solutions_found\": [               # Commentaires proposant des solutions\n",
    "        {\n",
    "            \"comment_id\": \"string\",\n",
    "            \"post_id\": \"string\",\n",
    "            \"author\": \"string\",\n",
    "            \"solution_text\": \"string\",\n",
    "            \"score\": 10\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None              # Message d'erreur si √©chec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c349f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les recommandations\n",
    "RECOMMENDATIONS_STRUCTURE = {\n",
    "    \"recommendations_success\": True,    # Bool√©en\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"analysis_summary\": \"string\",       # R√©sum√© de l'analyse de l'Agent 3\n",
    "    \"recommendations_timestamp\": \"2024-01-01 12:00:00\", # Date/heure\n",
    "    \"business_opportunities\": [         # Opportunit√©s business identifi√©es\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur\n",
    "            \"pain_score\": 45.6,         # Score de la douleur\n",
    "            \"opportunity_rank\": 1,      # Rang de l'opportunit√©\n",
    "            \"solutions\": [              # 3 solutions propos√©es\n",
    "                {\n",
    "                    \"type\": \"saas\",     # saas, digital_product, content, marketing\n",
    "                    \"title\": \"string\",  # Titre de la solution\n",
    "                    \"description\": \"string\", # Description d√©taill√©e\n",
    "                    \"complexity\": \"low\", # low, medium, high\n",
    "                    \"estimated_cost\": \"string\", # Co√ªt estim√©\n",
    "                    \"time_to_market\": \"string\" # Temps de d√©veloppement\n",
    "                }\n",
    "            ],\n",
    "            \"market_size\": \"string\",    # Taille du march√© estim√©e\n",
    "            \"competition_level\": \"low\"  # low, medium, high\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_opportunities\": [           # Top 3 des meilleures opportunit√©s\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"pain_score\": 45.6,\n",
    "            \"best_solution\": {\n",
    "                \"type\": \"saas\",\n",
    "                \"title\": \"string\",\n",
    "                \"description\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"export_available\": True,          # Si les exports sont disponibles\n",
    "    \"stored_solutions_count\": 5,       # Nombre de solutions stock√©es en SQLite\n",
    "    \"error_message\": None              # Message d'erreur si √©chec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5757db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def init_solutions_database() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Initialise la base de donn√©es SQLite pour stocker les commentaires avec solutions\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut de l'initialisation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Cr√©er la table des solutions\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS solutions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                comment_id TEXT UNIQUE,\n",
    "                post_id TEXT,\n",
    "                author TEXT,\n",
    "                solution_text TEXT,\n",
    "                score INTEGER,\n",
    "                pain_type TEXT,\n",
    "                intensity INTEGER,\n",
    "                subreddit TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"‚úÖ Base de donn√©es initialis√©e avec succ√®s\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def store_exceptional_solution(comment_data: str, pain_type: str, intensity: int) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Stocke un commentaire exceptionnel proposant une solution\n",
    "    \n",
    "    Args:\n",
    "        comment_data: Donn√©es du commentaire en JSON string\n",
    "        pain_type: Type de douleur identifi√©e\n",
    "        intensity: Intensit√© √©motionnelle (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut du stockage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comment = json.loads(comment_data)\n",
    "        \n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO solutions \n",
    "            (comment_id, post_id, author, solution_text, score, pain_type, intensity, subreddit)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            comment.get('id'),\n",
    "            comment.get('post_id'),\n",
    "            comment.get('author'),\n",
    "            comment.get('body'),\n",
    "            comment.get('score', 0),\n",
    "            pain_type,\n",
    "            intensity,\n",
    "            comment.get('subreddit', 'unknown')\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"‚úÖ Solution stock√©e: {comment.get('author', 'Unknown')}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def get_stored_solutions(subreddit: str = None, limit: int = 10) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re les solutions stock√©es en base\n",
    "    \n",
    "    Args:\n",
    "        subreddit: Filtrer par subreddit (optionnel)\n",
    "        limit: Nombre maximum de solutions √† r√©cup√©rer\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les solutions r√©cup√©r√©es\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if subreddit:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                WHERE subreddit = ? \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (subreddit, limit))\n",
    "        else:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (limit,))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        solutions = []\n",
    "        for row in rows:\n",
    "            solutions.append({\n",
    "                \"id\": row[0],\n",
    "                \"comment_id\": row[1],\n",
    "                \"post_id\": row[2],\n",
    "                \"author\": row[3],\n",
    "                \"solution_text\": row[4],\n",
    "                \"score\": row[5],\n",
    "                \"pain_type\": row[6],\n",
    "                \"intensity\": row[7],\n",
    "                \"subreddit\": row[8],\n",
    "                \"created_at\": row[9]\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"solutions\": solutions,\n",
    "            \"count\": len(solutions)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7b37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def export_final_report(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final avec analyse et recommandations\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Donn√©es de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Donn√©es des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parser les donn√©es JSON\n",
    "        analysis = json.loads(analysis_data)\n",
    "        recommendations = json.loads(recommendations_data)\n",
    "        \n",
    "        filename = f\"rapport_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        if format_type == \"pdf\":\n",
    "            return export_analysis_to_pdf(analysis, recommendations, filename)\n",
    "        elif format_type == \"csv\":\n",
    "            return export_analysis_to_csv(analysis, recommendations, filename)\n",
    "        elif format_type == \"txt\":\n",
    "            return export_analysis_to_txt(analysis, recommendations, filename)\n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non support√©\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_exceptional_solutions(format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte uniquement les solutions exceptionnelles stock√©es en base\n",
    "    \n",
    "    Args:\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # R√©cup√©rer toutes les solutions\n",
    "        solutions_result = get_stored_solutions(limit=1000)\n",
    "        \n",
    "        if not solutions_result[\"success\"]:\n",
    "            return {\"success\": False, \"error\": \"Impossible de r√©cup√©rer les solutions\"}\n",
    "        \n",
    "        solutions = solutions_result[\"solutions\"]\n",
    "        filename = f\"solutions_exceptionnelles_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if format_type == \"txt\":\n",
    "            filepath = export_dir / f\"{filename}.txt\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"SOLUTIONS EXCEPTIONNELLES REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Nombre de solutions: {len(solutions)}\\n\\n\")\n",
    "                \n",
    "                for i, solution in enumerate(solutions, 1):\n",
    "                    f.write(f\"SOLUTION {i}:\\n\")\n",
    "                    f.write(f\"Auteur: {solution['author']}\\n\")\n",
    "                    f.write(f\"Subreddit: r/{solution['subreddit']}\\n\")\n",
    "                    f.write(f\"Score: {solution['score']}\\n\")\n",
    "                    f.write(f\"Type de douleur: {solution['pain_type']}\\n\")\n",
    "                    f.write(f\"Intensit√©: {solution['intensity']}/10\\n\")\n",
    "                    f.write(f\"Date: {solution['created_at']}\\n\")\n",
    "                    f.write(f\"Solution:\\n{solution['solution_text']}\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"‚úÖ Solutions export√©es en TXT: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"csv\":\n",
    "            import csv\n",
    "            filepath = export_dir / f\"{filename}.csv\"\n",
    "            with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['Auteur', 'Subreddit', 'Score', 'Type de douleur', 'Intensit√©', 'Date', 'Solution'])\n",
    "                for solution in solutions:\n",
    "                    writer.writerow([\n",
    "                        solution['author'],\n",
    "                        solution['subreddit'],\n",
    "                        solution['score'],\n",
    "                        solution['pain_type'],\n",
    "                        solution['intensity'],\n",
    "                        solution['created_at'],\n",
    "                        solution['solution_text']\n",
    "                    ])\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"‚úÖ Solutions export√©es en CSV: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"pdf\":\n",
    "            from reportlab.lib.pagesizes import letter\n",
    "            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "            from reportlab.lib.styles import getSampleStyleSheet\n",
    "            \n",
    "            filepath = export_dir / f\"{filename}.pdf\"\n",
    "            doc = SimpleDocTemplate(str(filepath), pagesize=letter)\n",
    "            styles = getSampleStyleSheet()\n",
    "            story = []\n",
    "            \n",
    "            title = Paragraph(f\"SOLUTIONS EXCEPTIONNELLES REDDIT\", styles['Title'])\n",
    "            story.append(title)\n",
    "            story.append(Spacer(1, 12))\n",
    "            \n",
    "            for i, solution in enumerate(solutions, 1):\n",
    "                story.append(Paragraph(f\"<b>Solution {i}:</b>\", styles['Heading2']))\n",
    "                story.append(Paragraph(f\"Auteur: {solution['author']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Subreddit: r/{solution['subreddit']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Score: {solution['score']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Solution: {solution['solution_text']}\", styles['Normal']))\n",
    "                story.append(Spacer(1, 12))\n",
    "            \n",
    "            doc.build(story)\n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"‚úÖ Solutions export√©es en PDF: {filepath}\"}\n",
    "        \n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non support√©\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_both_reports(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final ET les solutions exceptionnelles\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Donn√©es de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Donn√©es des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et les chemins des fichiers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Exporter le rapport final\n",
    "        report_result = export_final_report(analysis_data, recommendations_data, format_type)\n",
    "        \n",
    "        # Exporter les solutions exceptionnelles\n",
    "        solutions_result = export_exceptional_solutions(format_type)\n",
    "        \n",
    "        if report_result[\"success\"] and solutions_result[\"success\"]:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"report_file\": report_result[\"filepath\"],\n",
    "                \"solutions_file\": solutions_result[\"filepath\"],\n",
    "                \"message\": \"‚úÖ Rapport final et solutions export√©s\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Erreur rapport: {report_result.get('error', 'OK')}, Erreur solutions: {solutions_result.get('error', 'OK')}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15a1be",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dabefe",
   "metadata": {},
   "source": [
    "#### Subreddit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0139a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def check_subreddit_exists(subreddit_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    V√©rifie si un subreddit existe via l'API PRAW\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit (sans le 'r/')\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les informations du subreddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Utiliser PRAW pour acc√©der au subreddit\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Essayer d'acc√©der aux informations du subreddit\n",
    "        # Cela va lever une exception si le subreddit n'existe pas\n",
    "        subreddit_info = {\n",
    "            \"exists\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"subscribers\": subreddit.subscribers,\n",
    "            \"description\": subreddit.public_description,\n",
    "            \"title\": subreddit.title,\n",
    "            \"is_private\": subreddit.subreddit_type == 'private',\n",
    "            \"created_utc\": datetime.fromtimestamp(subreddit.created_utc).strftime('%Y-%m-%d'),\n",
    "            \"url\": f\"https://reddit.com/r/{subreddit_name}\"\n",
    "        }\n",
    "        \n",
    "        return subreddit_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def calculate_pain_score(frequency: int, avg_upvotes: float, avg_comments: float, avg_intensity: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcule le score de priorit√© d'une douleur utilisateur\n",
    "    \n",
    "    Args:\n",
    "        frequency: Nombre de posts mentionnant cette douleur\n",
    "        avg_upvotes: Moyenne des upvotes des posts concern√©s\n",
    "        avg_comments: Moyenne des commentaires des posts concern√©s\n",
    "        avg_intensity: Intensit√© √©motionnelle moyenne (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le score calcul√© et les d√©tails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Formule de scoring\n",
    "        score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (avg_intensity * 0.3)\n",
    "        \n",
    "        # Normaliser l'intensit√© (1-10 vers 0-100)\n",
    "        normalized_intensity = avg_intensity * 10\n",
    "        \n",
    "        # Calculer les composantes pour le d√©tail\n",
    "        frequency_component = frequency * 0.4\n",
    "        upvotes_component = avg_upvotes * 0.2\n",
    "        comments_component = avg_comments * 0.1\n",
    "        intensity_component = normalized_intensity * 0.3\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"total_score\": round(score, 2),\n",
    "            \"components\": {\n",
    "                \"frequency_component\": round(frequency_component, 2),\n",
    "                \"upvotes_component\": round(upvotes_component, 2),\n",
    "                \"comments_component\": round(comments_component, 2),\n",
    "                \"intensity_component\": round(intensity_component, 2)\n",
    "            },\n",
    "            \"formula\": \"score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (intensity * 0.3)\",\n",
    "            \"input_values\": {\n",
    "                \"frequency\": frequency,\n",
    "                \"avg_upvotes\": avg_upvotes,\n",
    "                \"avg_comments\": avg_comments,\n",
    "                \"avg_intensity\": avg_intensity\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915eade",
   "metadata": {},
   "source": [
    "#### Reddit Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13721b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def scrape_subreddit_posts(subreddit_name: str, num_posts: int = 10, sort_criteria: str = \"top\", comments_limit: int = 10, time_filter: str = \"month\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts d'un subreddit selon les param√®tres donn√©s\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit\n",
    "        num_posts: Nombre de posts √† r√©cup√©rer (max 50)\n",
    "        sort_criteria: Crit√®re de tri (top, new, hot, best, rising)\n",
    "        comments_limit: Nombre de commentaires par post (max 50)\n",
    "        time_filter: P√©riode pour top/rising (hour, day, week, month, year, all)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les posts scrap√©s et m√©tadonn√©es\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limiter les valeurs\n",
    "        num_posts = min(num_posts, 50)\n",
    "        comments_limit = min(comments_limit, 50)\n",
    "        \n",
    "        # V√©rifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # V√©rifier si le crit√®re demand√© existe\n",
    "        original_criteria = sort_criteria\n",
    "        fallback_used = False\n",
    "        \n",
    "        # Tester seulement le crit√®re demand√©\n",
    "        if sort_criteria == \"top\" and hasattr(subreddit, 'top'):\n",
    "            sort_method = subreddit.top\n",
    "        elif sort_criteria == \"new\" and hasattr(subreddit, 'new'):\n",
    "            sort_method = subreddit.new\n",
    "        elif sort_criteria == \"hot\" and hasattr(subreddit, 'hot'):\n",
    "            sort_method = subreddit.hot\n",
    "        elif sort_criteria == \"best\" and hasattr(subreddit, 'best'):\n",
    "            sort_method = subreddit.best\n",
    "        elif sort_criteria == \"rising\" and hasattr(subreddit, 'rising'):\n",
    "            sort_method = subreddit.rising\n",
    "        else:\n",
    "            # Fallback sur \"new\" si le crit√®re demand√© n'existe pas\n",
    "            sort_criteria = \"new\"\n",
    "            sort_method = subreddit.new\n",
    "            fallback_used = True\n",
    "        \n",
    "        # R√©cup√©rer les posts\n",
    "        posts_data = []\n",
    "        \n",
    "        try:\n",
    "            if sort_criteria in [\"top\", \"rising\"]:\n",
    "                posts = sort_method(limit=num_posts, time_filter=time_filter)\n",
    "            else:\n",
    "                posts = sort_method(limit=num_posts)\n",
    "        except Exception as e:\n",
    "            # Si le crit√®re √©choue, essayer \"new\"\n",
    "            sort_criteria = \"new\"\n",
    "            posts = subreddit.new(limit=num_posts)\n",
    "            fallback_used = True\n",
    "        \n",
    "        for post in posts:\n",
    "            # R√©cup√©rer les commentaires\n",
    "            post.comments.replace_more(limit=5)\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:comments_limit]:\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"id\": comment.id\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:1000] + \"...\" if len(post.selftext) > 1000 else post.selftext,\n",
    "                \"comments\": comments_data,\n",
    "                \"id\": post.id\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria,\n",
    "            \"original_criteria\": original_criteria,\n",
    "            \"time_filter\": time_filter,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"comments_limit_per_post\": comments_limit,\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"fallback_used\": fallback_used\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e88e8",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b0d96",
   "metadata": {},
   "source": [
    "#### Agent 1 : Interaction Utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bcc915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1_interaction = Agent(\n",
    "    name=\"UserInteractionAgent\",\n",
    "    instructions=\"\"\"Tu es un assistant sp√©cialis√© dans l‚Äôanalyse de Reddit pour identifier \n",
    "    les points de douleur des entrepreneurs.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Saluer l'utilisateur avec politesse et professionnalisme.\n",
    "2. Expliquer clairement ta mission : analyser un subreddit pour d√©tecter les probl√®mes/frustrations r√©currents.\n",
    "3. Demander le nom dusubreddit √† analyser.\n",
    "4. Pr√©senter les 5 crit√®res de tri Reddit pour les posts, de fa√ßon concise et p√©dagogique :\n",
    "   - \"Top\" ‚Üí les posts avec le meilleur score sur une p√©riode (votes positifs - n√©gatifs)\n",
    "   - \"New\" ‚Üí les posts les plus r√©cents (ordre chronologique)\n",
    "   - \"Hot\" ‚Üí m√©lange du score + fra√Æcheur (post r√©cent et populaire)\n",
    "   - \"Best\" ‚Üí pertinence + vote + r√©ponse\n",
    "   - \"Rising\" ‚Üí posts r√©cents qui gagnent rapidement en popularit√©\n",
    "5. Demander les param√®tres d'analyse :\n",
    "   - Nombre de posts √† analyser\n",
    "   - Nombre de commentaires par post\n",
    "   - P√©riode souhait√©e (pour top ou rising)\n",
    "6. Si l'utilisateur ne sait pas, expliquer bri√®vement chaque param√®tre.\n",
    "7. Proposer des valeurs par d√©faut si n√©cessaire :\n",
    "\n",
    "Param√®tres par d√©faut:\n",
    "- Nombre de posts: 10\n",
    "- Nombre de commentaires par post: 10\n",
    "- Crit√®re: \"top\"\n",
    "- P√©riode: \"month\" (pour top/rising)\n",
    "\n",
    "8.Valider les choix de l‚Äôutilisateur, reformuler les param√®tres et demander confirmation.\n",
    "9.Retourner les param√®tres sous forme de JSON structur√©, pr√™t √† √™tre utilis√© par le scraper.\n",
    "10. Une fois que l'utilisateur fournit le subreddit, demande-lui si c'est bien le bon \n",
    "et si les param√®tres par d√©faut lui conviennent. S'il confirme, Handoff ensuite le json √† l'agent ScrapingAgent.\n",
    "\n",
    "STRUCTURE JSON √Ä RETOURNER:\n",
    "{json.dumps(USER_PARAMS_STRUCTURE, indent=2)}\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists\n",
    "    ],\n",
    "    handoffs=[agent_2_scraping],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4feabe",
   "metadata": {},
   "source": [
    "#### Agent 2 : Scraping des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "453d4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2_scraping = Agent(\n",
    "    name=\"ScrapingAgent\",\n",
    "    instructions=f\"\"\"Tu es un agent sp√©cialis√© dans le scraping de donn√©es Reddit.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Recevoir les param√®tres de l'Agent 1 (subreddit, nombre de posts, nombre de commentaires, crit√®re de tri, p√©riode)\n",
    "2. Scraper les donn√©es du subreddit en respectant fid√®lement ces param√®tres en utilisant l'outil scrape_subreddit_posts\n",
    "3. V√©rifier que les donn√©es ont bien √©t√© r√©cup√©r√©es (pas vides, structure correcte)\n",
    "4. Une fois toutes les donn√©es scrapp√©es, Retourner un objet JSON structur√© et handoff √† PainAnalysisAgent\n",
    "\n",
    "IMPORTANT - RESPECTER LES PARAM√àTRES RE√áUS:\n",
    "- Utilise EXACTEMENT le crit√®re de tri re√ßu (top, new, hot, best, rising)\n",
    "- Utilise EXACTEMENT le nombre de posts re√ßu\n",
    "- Utilise EXACTEMENT le nombre de commentaires re√ßu\n",
    "- Utilise EXACTEMENT la p√©riode re√ßue (pour top/rising)\n",
    "- Ne change JAMAIS ces param√®tres, m√™me si tu penses qu'un autre crit√®re serait mieux\n",
    "\n",
    "IMPORTANT - POUR LE HANDOFF:\n",
    "Apr√®s avoir termin√© le scraping et cr√©√© le JSON structur√©, tu DOIS faire un handoff vers l'agent PainAnalysisAgent \n",
    "en lui passant EXACTEMENT tes donn√©es scrap√©es dans le format JSON suivant.\n",
    "\n",
    "\n",
    "STRUCTURE JSON √Ä RETOURNER:\n",
    "{json.dumps(SCRAPED_DATA_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{{\n",
    "    \"scraping_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}}\n",
    "\n",
    "Sois rigoureux, pr√©cis et respecte TOUJOURS les param√®tres re√ßus.\"\"\",\n",
    "    tools=[\n",
    "        scrape_subreddit_posts\n",
    "    ],\n",
    "    handoffs=[agent_3_analysis],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612e230",
   "metadata": {},
   "source": [
    "#### Agent 3 : Analyse et Synth√®se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2f5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_3_analysis = Agent(\n",
    "    name=\"PainAnalysisAgent\",\n",
    "    instructions=f\"\"\"Tu es un expert analyste sp√©cialis√© dans l'identification des probl√®mes et \n",
    "    frustrations r√©currents des utilisateurs.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Analyser le sentiment et l‚Äôintensit√© √©motionnelle de chaque post et commentaire \n",
    "2. Identifier les douleurs r√©currentes, quelle que soit la langue\n",
    "3. Calculer le score de priorit√© de chaque douleur avec l'outil calculate_pain_score\n",
    "4. Classer les douleurs par ordre de priorit√©\n",
    "5. Rep√©rer les commentaires exceptionnels proposant des solutions concr√®tes\n",
    "6. Stocker ces solutions exceptionnelles dans la base SQLite avec l'outil store_exceptional_solution\n",
    "7. Fournir une analyse structur√©e et compl√®te\n",
    "8. Retourner un objet JSON structur√© et handoff √† RecommendationsAgent\n",
    "\n",
    "Crit√®res pour une solution exceptionnelle:\n",
    "- Score du commentaire > 10\n",
    "- Propose une solution concr√®te et r√©alisable\n",
    "- Solution d√©taill√©e et utile\n",
    "\n",
    "IMPORTANT - POUR LE HANDOFF:\n",
    "Apr√®s avoir termin√© ton analyse et cr√©√© le JSON structur√©, tu DOIS faire un handoff vers l'agent RecommendationsAgent \n",
    "en lui passant EXACTEMENT ton analyse dans le format JSON suivant.\n",
    "\n",
    "STRUCTURE JSON √Ä RETOURNER:\n",
    "{json.dumps(PAIN_ANALYSIS_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{{\n",
    "    \"analysis_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}}\n",
    "\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        calculate_pain_score,\n",
    "        init_solutions_database,\n",
    "        store_exceptional_solution,\n",
    "        get_stored_solutions\n",
    "    ],\n",
    "    handoffs=[agent_4_recommendations],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173fe2c",
   "metadata": {},
   "source": [
    "#### Agent 4 : Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b67314",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_4_recommendations = Agent(\n",
    "    name=\"RecommendationsAgent\",\n",
    "    instructions=f\"\"\"Tu es un expert en business development et cr√©ation d'entreprises.\n",
    "    Tu es cr√©atif, ing√©nieux, r√©aliste et propose des solutions concr√®tes et r√©alisables.\n",
    "\n",
    "Ton r√¥le est de:\n",
    "1. Analyser les douleurs identifi√©es par l'Agent 3\n",
    "2. Pour chaque douleur identifi√©e, propose 3 opportunit√©s business parmis:\n",
    "   - Solutions SaaS (priorit√©)\n",
    "   - Produits digitaux\n",
    "   - Cr√©ation de contenu\n",
    "   - Marketing/Formation\n",
    "3. Pour chaque opportunit√©, pr√©cise :\n",
    "   - Le type (saas, digital_product, content, marketing, etc.)\n",
    "   - Un titre clair\n",
    "   - Une description d√©taill√©e\n",
    "   - Le niveau de complexit√©\n",
    "   - Le co√ªt estim√©\n",
    "   - Le temps de d√©veloppement estim√©\n",
    "4. Classer les opportunit√©s business selon leur potentiel (rentabilit√© + faisabilit√©)\n",
    "5. Pr√©sente le tout de fa√ßon claire et conversationnelle dans le chat (pas en JSON brut)\n",
    "6. Proposer les exports disponibles avec les diff√©rents outils :\n",
    "   - export_final_report: Rapport complet (analyse + recommandations)\n",
    "   - export_exceptional_solutions: Seulement les commentaires exceptionnels\n",
    "   - export_both_reports: Les deux rapports\n",
    "7. Rester disponible pour r√©pondre aux questions de l'utilisateur ou g√©n√©rer un export\n",
    "\n",
    "\n",
    "FORMAT DE R√âPONSE :\n",
    "- Pour chaque douleur, liste les 3 opportunit√©s business propos√©es, avec leurs d√©tails\n",
    "- √Ä la fin, propose les options d'export disponibles\n",
    "- Invite l'utilisateur √† demander un export, poser des questions, ou relancer une analyse sur un autre subreddit\n",
    "\n",
    "Exemple de structure :\n",
    "\"Pour la douleur : [Nom de la douleur]\n",
    "1. [Opportunit√© 1] (SaaS) - Complexit√©: [niveau], Co√ªt: [estimation], Temps: [estimation]\n",
    "   Description : ...\n",
    "2. [Opportunit√© 2] (Produit digital) - ...\n",
    "   Description : ...\n",
    "3. [Opportunit√© 3] (Marketing/Formation) - ...\n",
    "   Description : ...\n",
    "\n",
    "[... R√©p√©ter pour chaque douleur identifi√©e ...]\n",
    "\n",
    "üìä EXPORTS DISPONIBLES :\n",
    "- Rapport complet (PDF/CSV/TXT)\n",
    "- Solutions exceptionnelles uniquement\n",
    "- Les deux rapports\n",
    "\n",
    "Que souhaitez-vous faire ? Voulez-vous un export, avez-vous des questions, ou souhaitez-vous analyser un autre subreddit ?\"\n",
    "\n",
    "R√àGLE :\n",
    "- Reste conversationnel et disponible pour continuer l'√©change.\n",
    "- Si l'utilisateur veut analyser un autre subreddit, transf√®re la main √† l'Agent 1 (UserInteractionAgent) et explique-le clairement √† l'utilisateur.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        get_stored_solutions,\n",
    "        export_final_report,\n",
    "        export_exceptional_solutions,\n",
    "        export_both_reports\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d480e",
   "metadata": {},
   "source": [
    "# TEST Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7945717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Junca\\projects\\Projet_Reddit\\Backend\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interface Gradio avec async correct\n",
    "import gradio as gr\n",
    "from agents import Runner, trace\n",
    "\n",
    "async def simple_chat(message, history):\n",
    "    \"\"\"\n",
    "    Fonction de chat avec async\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construire le contexte avec l'historique\n",
    "        context = \"\"\n",
    "        if history:\n",
    "            for user_msg, assistant_msg in history:\n",
    "                context += f\"Humain: {user_msg}\\n\"\n",
    "                context += f\"Assistant: {assistant_msg}\\n\"\n",
    "        \n",
    "        # Ajouter le message actuel\n",
    "        context += f\"Humain: {message}\\nAssistant: \"\n",
    "        \n",
    "        # Lancer l'Agent 1\n",
    "        result = await Runner.run(agent_1_interaction, context)\n",
    "        \n",
    "        # Extraire la r√©ponse\n",
    "        if hasattr(result, 'final_output'):\n",
    "            response_text = str(result.final_output)\n",
    "        else:\n",
    "            response_text = str(result)\n",
    "        \n",
    "        return response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Erreur: {str(e)}\"\n",
    "\n",
    "# Interface simple avec gr.ChatInterface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=simple_chat,\n",
    "    title=\"ü§ñ Analyse Reddit - Identification des Douleurs Utilisateurs\",\n",
    "    description=\"D√©couvrez les probl√®mes r√©currents sur Reddit et trouvez des opportunit√©s business\",\n",
    "    examples=[\n",
    "        \"Je veux analyser le subreddit entrepreneur\",\n",
    "        \"Peux-tu m'expliquer les crit√®res de tri Reddit ?\",\n",
    "        \"Analysez le subreddit programming avec 20 posts\"\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Lancer\n",
    "demo.launch(share=False, quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test Kernel",
   "language": "python",
   "name": "test-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
