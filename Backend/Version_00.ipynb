{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64df690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import sqlite3\n",
    "import praw\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, function_tool, trace, WebSearchTool\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a3af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = \"RedditScraper/1.0\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "Path(\"temp\").mkdir(exist_ok=True)\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ef1b8",
   "metadata": {},
   "source": [
    "# Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6257a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les paramètres utilisateur\n",
    "USER_PARAMS_STRUCTURE = {\n",
    "    \"subreddit_name\": \"string\",  # Nom du subreddit\n",
    "    \"num_posts\": 10,             # Nombre de posts à scraper (max 50)\n",
    "    \"comments_limit\": 10,        # Nombre de commentaires par post (max 50)\n",
    "    \"sort_criteria\": \"top\",      # top, new, hot, best, rising\n",
    "    \"time_filter\": \"month\",      # hour, day, week, month, year, all (pour top/rising)\n",
    "    \"confirmed\": True,           # Validation utilisateur\n",
    "    \"timestamp\": \"2024-01-01 12:00:00\",  # Date/heure de la demande\n",
    "    \"user_preferences\": {        # Préférences utilisateur\n",
    "        \"language\": \"auto\",      # Langue d'analyse\n",
    "        \"export_format\": \"all\"   # Format d'export préféré\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les données scrapées\n",
    "SCRAPED_DATA_STRUCTURE = {\n",
    "    \"scraping_success\": True,           # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"posts_count\": 10,                  # Nombre de posts récupérés\n",
    "    \"total_comments\": 100,              # Total des commentaires\n",
    "    \"scraped_at\": \"2024-01-01 12:00:00\", # Date/heure du scraping\n",
    "    \"parameters_used\": {                # Paramètres utilisés\n",
    "        \"sort_criteria\": \"top\",         # Critère de tri\n",
    "        \"time_filter\": \"month\",         # Filtre temporel\n",
    "        \"comments_limit\": 10            # Limite de commentaires\n",
    "    },\n",
    "    \"posts\": [                          # Liste des posts\n",
    "        {\n",
    "            \"title\": \"string\",          # Titre du post\n",
    "            \"author\": \"string\",         # Auteur\n",
    "            \"score\": 100,               # Score (upvotes)\n",
    "            \"upvote_ratio\": 0.95,       # Ratio upvote\n",
    "            \"num_comments\": 50,         # Nombre de commentaires\n",
    "            \"created_utc\": \"2024-01-01 10:00:00\", # Date création\n",
    "            \"url\": \"https://reddit.com/...\", # URL du post\n",
    "            \"selftext\": \"string\",       # Contenu du post\n",
    "            \"id\": \"string\",             # ID du post\n",
    "            \"comments\": [               # Liste des commentaires\n",
    "                {\n",
    "                    \"author\": \"string\", # Auteur du commentaire\n",
    "                    \"body\": \"string\",   # Contenu du commentaire\n",
    "                    \"score\": 10,        # Score du commentaire\n",
    "                    \"created_utc\": \"2024-01-01 10:30:00\", # Date création\n",
    "                    \"id\": \"string\"      # ID du commentaire\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None               # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67583e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour l'analyse des douleurs\n",
    "PAIN_ANALYSIS_STRUCTURE = {\n",
    "    \"analysis_success\": True,           # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit analysé\n",
    "    \"posts_analyzed\": 10,               # Nombre de posts analysés\n",
    "    \"total_comments_analyzed\": 100,     # Total des commentaires analysés\n",
    "    \"analysis_timestamp\": \"2024-01-01 12:00:00\", # Date/heure de l'analyse\n",
    "    \"pain_categories\": [                # Liste des catégories de douleurs\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur (ex: \"technique\", \"financier\")\n",
    "            \"frequency\": 5,             # Nombre de posts mentionnant cette douleur\n",
    "            \"avg_upvotes\": 25.5,        # Moyenne des upvotes\n",
    "            \"avg_comments\": 12.3,       # Moyenne des commentaires\n",
    "            \"avg_intensity\": 7.2,       # Intensité émotionnelle moyenne (1-10)\n",
    "            \"score\": 45.6,              # Score de priorité calculé\n",
    "            \"rank\": 1,                  # Rang dans le classement\n",
    "            \"descriptions\": [           # Exemples de descriptions\n",
    "                \"Description 1\",\n",
    "                \"Description 2\"\n",
    "            ],\n",
    "            \"posts_affected\": [         # IDs des posts concernés\n",
    "                \"post_id_1\",\n",
    "                \"post_id_2\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_pains\": [                   # Top 3 des douleurs prioritaires\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"score\": 45.6,\n",
    "            \"frequency\": 5,\n",
    "            \"avg_intensity\": 7.2,\n",
    "            \"summary\": \"Résumé de la douleur\"\n",
    "        }\n",
    "    ],\n",
    "    \"overall_summary\": \"string\",        # Résumé global de l'analyse\n",
    "    \"solutions_found\": [               # Commentaires proposant des solutions\n",
    "        {\n",
    "            \"comment_id\": \"string\",\n",
    "            \"post_id\": \"string\",\n",
    "            \"author\": \"string\",\n",
    "            \"solution_text\": \"string\",\n",
    "            \"score\": 10\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None              # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c349f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les recommandations\n",
    "RECOMMENDATIONS_STRUCTURE = {\n",
    "    \"recommendations_success\": True,    # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"analysis_summary\": \"string\",       # Résumé de l'analyse de l'Agent 3\n",
    "    \"recommendations_timestamp\": \"2024-01-01 12:00:00\", # Date/heure\n",
    "    \"business_opportunities\": [         # Opportunités business identifiées\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur\n",
    "            \"pain_score\": 45.6,         # Score de la douleur\n",
    "            \"opportunity_rank\": 1,      # Rang de l'opportunité\n",
    "            \"solutions\": [              # 3 solutions proposées\n",
    "                {\n",
    "                    \"type\": \"saas\",     # saas, digital_product, content, marketing\n",
    "                    \"title\": \"string\",  # Titre de la solution\n",
    "                    \"description\": \"string\", # Description détaillée\n",
    "                    \"complexity\": \"low\", # low, medium, high\n",
    "                    \"estimated_cost\": \"string\", # Coût estimé\n",
    "                    \"time_to_market\": \"string\" # Temps de développement\n",
    "                }\n",
    "            ],\n",
    "            \"market_size\": \"string\",    # Taille du marché estimée\n",
    "            \"competition_level\": \"low\"  # low, medium, high\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_opportunities\": [           # Top 3 des meilleures opportunités\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"pain_score\": 45.6,\n",
    "            \"best_solution\": {\n",
    "                \"type\": \"saas\",\n",
    "                \"title\": \"string\",\n",
    "                \"description\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"export_available\": True,          # Si les exports sont disponibles\n",
    "    \"stored_solutions_count\": 5,       # Nombre de solutions stockées en SQLite\n",
    "    \"error_message\": None              # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f171ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def export_final_report(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final avec analyse et recommandations\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parser les données JSON\n",
    "        analysis = json.loads(analysis_data)\n",
    "        recommendations = json.loads(recommendations_data)\n",
    "        \n",
    "        filename = f\"rapport_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        if format_type == \"pdf\":\n",
    "            return export_analysis_to_pdf(analysis, recommendations, filename)\n",
    "        elif format_type == \"csv\":\n",
    "            return export_analysis_to_csv(analysis, recommendations, filename)\n",
    "        elif format_type == \"txt\":\n",
    "            return export_analysis_to_txt(analysis, recommendations, filename)\n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_exceptional_solutions(format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte uniquement les solutions exceptionnelles stockées en base\n",
    "    \n",
    "    Args:\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupérer toutes les solutions\n",
    "        solutions_result = get_stored_solutions(limit=1000)\n",
    "        \n",
    "        if not solutions_result[\"success\"]:\n",
    "            return {\"success\": False, \"error\": \"Impossible de récupérer les solutions\"}\n",
    "        \n",
    "        solutions = solutions_result[\"solutions\"]\n",
    "        filename = f\"solutions_exceptionnelles_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if format_type == \"txt\":\n",
    "            filepath = export_dir / f\"{filename}.txt\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"SOLUTIONS EXCEPTIONNELLES REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Nombre de solutions: {len(solutions)}\\n\\n\")\n",
    "                \n",
    "                for i, solution in enumerate(solutions, 1):\n",
    "                    f.write(f\"SOLUTION {i}:\\n\")\n",
    "                    f.write(f\"Auteur: {solution['author']}\\n\")\n",
    "                    f.write(f\"Subreddit: r/{solution['subreddit']}\\n\")\n",
    "                    f.write(f\"Score: {solution['score']}\\n\")\n",
    "                    f.write(f\"Type de douleur: {solution['pain_type']}\\n\")\n",
    "                    f.write(f\"Intensité: {solution['intensity']}/10\\n\")\n",
    "                    f.write(f\"Date: {solution['created_at']}\\n\")\n",
    "                    f.write(f\"Solution:\\n{solution['solution_text']}\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en TXT: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"csv\":\n",
    "            import csv\n",
    "            filepath = export_dir / f\"{filename}.csv\"\n",
    "            with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['Auteur', 'Subreddit', 'Score', 'Type de douleur', 'Intensité', 'Date', 'Solution'])\n",
    "                for solution in solutions:\n",
    "                    writer.writerow([\n",
    "                        solution['author'],\n",
    "                        solution['subreddit'],\n",
    "                        solution['score'],\n",
    "                        solution['pain_type'],\n",
    "                        solution['intensity'],\n",
    "                        solution['created_at'],\n",
    "                        solution['solution_text']\n",
    "                    ])\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en CSV: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"pdf\":\n",
    "            from reportlab.lib.pagesizes import letter\n",
    "            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "            from reportlab.lib.styles import getSampleStyleSheet\n",
    "            \n",
    "            filepath = export_dir / f\"{filename}.pdf\"\n",
    "            doc = SimpleDocTemplate(str(filepath), pagesize=letter)\n",
    "            styles = getSampleStyleSheet()\n",
    "            story = []\n",
    "            \n",
    "            title = Paragraph(f\"SOLUTIONS EXCEPTIONNELLES REDDIT\", styles['Title'])\n",
    "            story.append(title)\n",
    "            story.append(Spacer(1, 12))\n",
    "            \n",
    "            for i, solution in enumerate(solutions, 1):\n",
    "                story.append(Paragraph(f\"<b>Solution {i}:</b>\", styles['Heading2']))\n",
    "                story.append(Paragraph(f\"Auteur: {solution['author']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Subreddit: r/{solution['subreddit']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Score: {solution['score']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Solution: {solution['solution_text']}\", styles['Normal']))\n",
    "                story.append(Spacer(1, 12))\n",
    "            \n",
    "            doc.build(story)\n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en PDF: {filepath}\"}\n",
    "        \n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_both_reports(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final ET les solutions exceptionnelles\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et les chemins des fichiers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Exporter le rapport final\n",
    "        report_result = export_final_report(analysis_data, recommendations_data, format_type)\n",
    "        \n",
    "        # Exporter les solutions exceptionnelles\n",
    "        solutions_result = export_exceptional_solutions(format_type)\n",
    "        \n",
    "        if report_result[\"success\"] and solutions_result[\"success\"]:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"report_file\": report_result[\"filepath\"],\n",
    "                \"solutions_file\": solutions_result[\"filepath\"],\n",
    "                \"message\": \"✅ Rapport final et solutions exportés\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Erreur rapport: {report_result.get('error', 'OK')}, Erreur solutions: {solutions_result.get('error', 'OK')}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f5757db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def init_solutions_database() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Initialise la base de données SQLite pour stocker les commentaires avec solutions\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut de l'initialisation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Créer la table des solutions\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS solutions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                comment_id TEXT UNIQUE,\n",
    "                post_id TEXT,\n",
    "                author TEXT,\n",
    "                solution_text TEXT,\n",
    "                score INTEGER,\n",
    "                pain_type TEXT,\n",
    "                intensity INTEGER,\n",
    "                subreddit TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"✅ Base de données initialisée avec succès\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def store_exceptional_solution(comment_data: str, pain_type: str, intensity: int) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Stocke un commentaire exceptionnel proposant une solution\n",
    "    \n",
    "    Args:\n",
    "        comment_data: Données du commentaire en JSON string\n",
    "        pain_type: Type de douleur identifiée\n",
    "        intensity: Intensité émotionnelle (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut du stockage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comment = json.loads(comment_data)\n",
    "        \n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO solutions \n",
    "            (comment_id, post_id, author, solution_text, score, pain_type, intensity, subreddit)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            comment.get('id'),\n",
    "            comment.get('post_id'),\n",
    "            comment.get('author'),\n",
    "            comment.get('body'),\n",
    "            comment.get('score', 0),\n",
    "            pain_type,\n",
    "            intensity,\n",
    "            comment.get('subreddit', 'unknown')\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"✅ Solution stockée: {comment.get('author', 'Unknown')}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def get_stored_solutions(subreddit: str = None, limit: int = 10) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Récupère les solutions stockées en base\n",
    "    \n",
    "    Args:\n",
    "        subreddit: Filtrer par subreddit (optionnel)\n",
    "        limit: Nombre maximum de solutions à récupérer\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les solutions récupérées\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if subreddit:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                WHERE subreddit = ? \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (subreddit, limit))\n",
    "        else:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (limit,))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        solutions = []\n",
    "        for row in rows:\n",
    "            solutions.append({\n",
    "                \"id\": row[0],\n",
    "                \"comment_id\": row[1],\n",
    "                \"post_id\": row[2],\n",
    "                \"author\": row[3],\n",
    "                \"solution_text\": row[4],\n",
    "                \"score\": row[5],\n",
    "                \"pain_type\": row[6],\n",
    "                \"intensity\": row[7],\n",
    "                \"subreddit\": row[8],\n",
    "                \"created_at\": row[9]\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"solutions\": solutions,\n",
    "            \"count\": len(solutions)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7b37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def export_final_report(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final avec analyse et recommandations\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parser les données JSON\n",
    "        analysis = json.loads(analysis_data)\n",
    "        recommendations = json.loads(recommendations_data)\n",
    "        \n",
    "        filename = f\"rapport_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        if format_type == \"pdf\":\n",
    "            return export_analysis_to_pdf(analysis, recommendations, filename)\n",
    "        elif format_type == \"csv\":\n",
    "            return export_analysis_to_csv(analysis, recommendations, filename)\n",
    "        elif format_type == \"txt\":\n",
    "            return export_analysis_to_txt(analysis, recommendations, filename)\n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_exceptional_solutions(format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte uniquement les solutions exceptionnelles stockées en base\n",
    "    \n",
    "    Args:\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupérer toutes les solutions\n",
    "        solutions_result = get_stored_solutions(limit=1000)\n",
    "        \n",
    "        if not solutions_result[\"success\"]:\n",
    "            return {\"success\": False, \"error\": \"Impossible de récupérer les solutions\"}\n",
    "        \n",
    "        solutions = solutions_result[\"solutions\"]\n",
    "        filename = f\"solutions_exceptionnelles_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if format_type == \"txt\":\n",
    "            filepath = export_dir / f\"{filename}.txt\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"SOLUTIONS EXCEPTIONNELLES REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Nombre de solutions: {len(solutions)}\\n\\n\")\n",
    "                \n",
    "                for i, solution in enumerate(solutions, 1):\n",
    "                    f.write(f\"SOLUTION {i}:\\n\")\n",
    "                    f.write(f\"Auteur: {solution['author']}\\n\")\n",
    "                    f.write(f\"Subreddit: r/{solution['subreddit']}\\n\")\n",
    "                    f.write(f\"Score: {solution['score']}\\n\")\n",
    "                    f.write(f\"Type de douleur: {solution['pain_type']}\\n\")\n",
    "                    f.write(f\"Intensité: {solution['intensity']}/10\\n\")\n",
    "                    f.write(f\"Date: {solution['created_at']}\\n\")\n",
    "                    f.write(f\"Solution:\\n{solution['solution_text']}\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en TXT: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"csv\":\n",
    "            import csv\n",
    "            filepath = export_dir / f\"{filename}.csv\"\n",
    "            with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['Auteur', 'Subreddit', 'Score', 'Type de douleur', 'Intensité', 'Date', 'Solution'])\n",
    "                for solution in solutions:\n",
    "                    writer.writerow([\n",
    "                        solution['author'],\n",
    "                        solution['subreddit'],\n",
    "                        solution['score'],\n",
    "                        solution['pain_type'],\n",
    "                        solution['intensity'],\n",
    "                        solution['created_at'],\n",
    "                        solution['solution_text']\n",
    "                    ])\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en CSV: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"pdf\":\n",
    "            from reportlab.lib.pagesizes import letter\n",
    "            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "            from reportlab.lib.styles import getSampleStyleSheet\n",
    "            \n",
    "            filepath = export_dir / f\"{filename}.pdf\"\n",
    "            doc = SimpleDocTemplate(str(filepath), pagesize=letter)\n",
    "            styles = getSampleStyleSheet()\n",
    "            story = []\n",
    "            \n",
    "            title = Paragraph(f\"SOLUTIONS EXCEPTIONNELLES REDDIT\", styles['Title'])\n",
    "            story.append(title)\n",
    "            story.append(Spacer(1, 12))\n",
    "            \n",
    "            for i, solution in enumerate(solutions, 1):\n",
    "                story.append(Paragraph(f\"<b>Solution {i}:</b>\", styles['Heading2']))\n",
    "                story.append(Paragraph(f\"Auteur: {solution['author']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Subreddit: r/{solution['subreddit']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Score: {solution['score']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Solution: {solution['solution_text']}\", styles['Normal']))\n",
    "                story.append(Spacer(1, 12))\n",
    "            \n",
    "            doc.build(story)\n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en PDF: {filepath}\"}\n",
    "        \n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_both_reports(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final ET les solutions exceptionnelles\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et les chemins des fichiers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Exporter le rapport final\n",
    "        report_result = export_final_report(analysis_data, recommendations_data, format_type)\n",
    "        \n",
    "        # Exporter les solutions exceptionnelles\n",
    "        solutions_result = export_exceptional_solutions(format_type)\n",
    "        \n",
    "        if report_result[\"success\"] and solutions_result[\"success\"]:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"report_file\": report_result[\"filepath\"],\n",
    "                \"solutions_file\": solutions_result[\"filepath\"],\n",
    "                \"message\": \"✅ Rapport final et solutions exportés\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Erreur rapport: {report_result.get('error', 'OK')}, Erreur solutions: {solutions_result.get('error', 'OK')}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15a1be",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dabefe",
   "metadata": {},
   "source": [
    "#### Subreddit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0139a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def check_subreddit_exists(subreddit_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vérifie si un subreddit existe via l'API PRAW\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit (sans le 'r/')\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les informations du subreddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Utiliser PRAW pour accéder au subreddit\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Essayer d'accéder aux informations du subreddit\n",
    "        # Cela va lever une exception si le subreddit n'existe pas\n",
    "        subreddit_info = {\n",
    "            \"exists\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"subscribers\": subreddit.subscribers,\n",
    "            \"description\": subreddit.public_description,\n",
    "            \"title\": subreddit.title,\n",
    "            \"is_private\": subreddit.subreddit_type == 'private',\n",
    "            \"created_utc\": datetime.fromtimestamp(subreddit.created_utc).strftime('%Y-%m-%d'),\n",
    "            \"url\": f\"https://reddit.com/r/{subreddit_name}\"\n",
    "        }\n",
    "        \n",
    "        return subreddit_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def calculate_pain_score(frequency: int, avg_upvotes: float, avg_comments: float, avg_intensity: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcule le score de priorité d'une douleur utilisateur\n",
    "    \n",
    "    Args:\n",
    "        frequency: Nombre de posts mentionnant cette douleur\n",
    "        avg_upvotes: Moyenne des upvotes des posts concernés\n",
    "        avg_comments: Moyenne des commentaires des posts concernés\n",
    "        avg_intensity: Intensité émotionnelle moyenne (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le score calculé et les détails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Formule de scoring\n",
    "        score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (avg_intensity * 0.3)\n",
    "        \n",
    "        # Normaliser l'intensité (1-10 vers 0-100)\n",
    "        normalized_intensity = avg_intensity * 10\n",
    "        \n",
    "        # Calculer les composantes pour le détail\n",
    "        frequency_component = frequency * 0.4\n",
    "        upvotes_component = avg_upvotes * 0.2\n",
    "        comments_component = avg_comments * 0.1\n",
    "        intensity_component = normalized_intensity * 0.3\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"total_score\": round(score, 2),\n",
    "            \"components\": {\n",
    "                \"frequency_component\": round(frequency_component, 2),\n",
    "                \"upvotes_component\": round(upvotes_component, 2),\n",
    "                \"comments_component\": round(comments_component, 2),\n",
    "                \"intensity_component\": round(intensity_component, 2)\n",
    "            },\n",
    "            \"formula\": \"score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (intensity * 0.3)\",\n",
    "            \"input_values\": {\n",
    "                \"frequency\": frequency,\n",
    "                \"avg_upvotes\": avg_upvotes,\n",
    "                \"avg_comments\": avg_comments,\n",
    "                \"avg_intensity\": avg_intensity\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915eade",
   "metadata": {},
   "source": [
    "#### Reddit Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7788bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def scrape_subreddit_posts(subreddit_name: str, num_posts: int = 10, sort_criteria: str = \"top\", comments_limit: int = 10, time_filter: str = \"month\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts d'un subreddit selon les paramètres donnés\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit\n",
    "        num_posts: Nombre de posts à récupérer (max 50)\n",
    "        sort_criteria: Critère de tri (top, new, hot, best, rising)\n",
    "        comments_limit: Nombre de commentaires par post (max 50)\n",
    "        time_filter: Période pour top/rising (hour, day, week, month, year, all)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les posts scrapés et métadonnées\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limiter les valeurs\n",
    "        num_posts = min(num_posts, 50)\n",
    "        comments_limit = min(comments_limit, 50)\n",
    "        \n",
    "        # Vérifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Mapper les critères de tri\n",
    "        sort_methods = {\n",
    "            \"top\": subreddit.top,\n",
    "            \"new\": subreddit.new,\n",
    "            \"hot\": subreddit.hot,\n",
    "            \"best\": subreddit.best,\n",
    "            \"rising\": subreddit.rising\n",
    "        }\n",
    "        \n",
    "        if sort_criteria not in sort_methods:\n",
    "            sort_criteria = \"top\"\n",
    "        \n",
    "        # Récupérer les posts\n",
    "        posts_data = []\n",
    "        \n",
    "        if sort_criteria in [\"top\", \"rising\"]:\n",
    "            posts = sort_methods[sort_criteria](limit=num_posts, time_filter=time_filter)\n",
    "        else:\n",
    "            posts = sort_methods[sort_criteria](limit=num_posts)\n",
    "        \n",
    "        for post in posts:\n",
    "            # Récupérer les commentaires\n",
    "            post.comments.replace_more(limit=5)\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:comments_limit]:\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"id\": comment.id\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:1000] + \"...\" if len(post.selftext) > 1000 else post.selftext,\n",
    "                \"comments\": comments_data,\n",
    "                \"id\": post.id\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria,\n",
    "            \"time_filter\": time_filter,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"comments_limit_per_post\": comments_limit,\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13721b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def scrape_subreddit_posts(subreddit_name: str, num_posts: int = 10, sort_criteria: str = \"top\", comments_limit: int = 10, time_filter: str = \"month\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts d'un subreddit selon les paramètres donnés\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit\n",
    "        num_posts: Nombre de posts à récupérer (max 50)\n",
    "        sort_criteria: Critère de tri (top, new, hot, best, rising)\n",
    "        comments_limit: Nombre de commentaires par post (max 50)\n",
    "        time_filter: Période pour top/rising (hour, day, week, month, year, all)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les posts scrapés et métadonnées\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limiter les valeurs\n",
    "        num_posts = min(num_posts, 50)\n",
    "        comments_limit = min(comments_limit, 50)\n",
    "        \n",
    "        # Vérifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Vérifier si le critère demandé existe\n",
    "        original_criteria = sort_criteria\n",
    "        fallback_used = False\n",
    "        \n",
    "        # Tester seulement le critère demandé\n",
    "        if sort_criteria == \"top\" and hasattr(subreddit, 'top'):\n",
    "            sort_method = subreddit.top\n",
    "        elif sort_criteria == \"new\" and hasattr(subreddit, 'new'):\n",
    "            sort_method = subreddit.new\n",
    "        elif sort_criteria == \"hot\" and hasattr(subreddit, 'hot'):\n",
    "            sort_method = subreddit.hot\n",
    "        elif sort_criteria == \"best\" and hasattr(subreddit, 'best'):\n",
    "            sort_method = subreddit.best\n",
    "        elif sort_criteria == \"rising\" and hasattr(subreddit, 'rising'):\n",
    "            sort_method = subreddit.rising\n",
    "        else:\n",
    "            # Fallback sur \"new\" si le critère demandé n'existe pas\n",
    "            sort_criteria = \"new\"\n",
    "            sort_method = subreddit.new\n",
    "            fallback_used = True\n",
    "        \n",
    "        # Récupérer les posts\n",
    "        posts_data = []\n",
    "        \n",
    "        try:\n",
    "            if sort_criteria in [\"top\", \"rising\"]:\n",
    "                posts = sort_method(limit=num_posts, time_filter=time_filter)\n",
    "            else:\n",
    "                posts = sort_method(limit=num_posts)\n",
    "        except Exception as e:\n",
    "            # Si le critère échoue, essayer \"new\"\n",
    "            sort_criteria = \"new\"\n",
    "            posts = subreddit.new(limit=num_posts)\n",
    "            fallback_used = True\n",
    "        \n",
    "        for post in posts:\n",
    "            # Récupérer les commentaires\n",
    "            post.comments.replace_more(limit=5)\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:comments_limit]:\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"id\": comment.id\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:1000] + \"...\" if len(post.selftext) > 1000 else post.selftext,\n",
    "                \"comments\": comments_data,\n",
    "                \"id\": post.id\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria,\n",
    "            \"original_criteria\": original_criteria,\n",
    "            \"time_filter\": time_filter,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"comments_limit_per_post\": comments_limit,\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"fallback_used\": fallback_used\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e88e8",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b0d96",
   "metadata": {},
   "source": [
    "#### Agent 1 : Interaction Utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bcc915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1_interaction = Agent(\n",
    "    name=\"UserInteractionAgent\",\n",
    "    instructions=\"\"\"Tu es un assistant spécialisé dans l’analyse de Reddit pour identifier \n",
    "    les points de douleur des entrepreneurs.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Saluer l'utilisateur avec politesse et professionnalisme.\n",
    "2. Expliquer clairement ta mission : analyser un subreddit pour détecter les problèmes/frustrations récurrents.\n",
    "3. Demander le nom dusubreddit à analyser.\n",
    "4. Présenter les 5 critères de tri Reddit pour les posts, de façon concise et pédagogique :\n",
    "   - \"Top\" → les posts avec le meilleur score sur une période (votes positifs - négatifs)\n",
    "   - \"New\" → les posts les plus récents (ordre chronologique)\n",
    "   - \"Hot\" → mélange du score + fraîcheur (post récent et populaire)\n",
    "   - \"Best\" → pertinence + vote + réponse\n",
    "   - \"Rising\" → posts récents qui gagnent rapidement en popularité\n",
    "5. Demander les paramètres d'analyse :\n",
    "   - Nombre de posts à analyser\n",
    "   - Nombre de commentaires par post\n",
    "   - Période souhaitée (pour top ou rising)\n",
    "6. Si l'utilisateur ne sait pas, expliquer brièvement chaque paramètre.\n",
    "7. Proposer des valeurs par défaut si nécessaire :\n",
    "\n",
    "Paramètres par défaut:\n",
    "- Nombre de posts: 10\n",
    "- Nombre de commentaires par post: 10\n",
    "- Critère: \"top\"\n",
    "- Période: \"month\" (pour top/rising)\n",
    "\n",
    "8.Valider les choix de l’utilisateur, reformuler les paramètres et demander confirmation.\n",
    "9.Retourner les paramètres sous forme de JSON structuré, prêt à être utilisé par le scraper.\n",
    "10. Une fois que l'utilisateur fournit le subreddit, demande-lui si c'est bien le bon \n",
    "et si les paramètres par défaut lui conviennent. S'il confirme, Handoff ensuite le json à l'agent ScrapingAgent.\n",
    "\n",
    "STRUCTURE JSON À RETOURNER:\n",
    "{json.dumps(USER_PARAMS_STRUCTURE, indent=2)}\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists\n",
    "    ],\n",
    "    handoffs=[agent_2_scraping],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4feabe",
   "metadata": {},
   "source": [
    "#### Agent 2 : Scraping des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "453d4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2_scraping = Agent(\n",
    "    name=\"ScrapingAgent\",\n",
    "    instructions=f\"\"\"Tu es un agent spécialisé dans le scraping de données Reddit.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Recevoir les paramètres de l'Agent 1 (subreddit, nombre de posts, nombre de commentaires, critère de tri, période)\n",
    "2. Scraper les données du subreddit en respectant fidèlement ces paramètres en utilisant l'outil scrape_subreddit_posts\n",
    "3. Vérifier que les données ont bien été récupérées (pas vides, structure correcte)\n",
    "4. Une fois toutes les données scrappées, Retourner un objet JSON structuré et handoff à PainAnalysisAgent\n",
    "\n",
    "IMPORTANT - RESPECTER LES PARAMÈTRES REÇUS:\n",
    "- Utilise EXACTEMENT le critère de tri reçu (top, new, hot, best, rising)\n",
    "- Utilise EXACTEMENT le nombre de posts reçu\n",
    "- Utilise EXACTEMENT le nombre de commentaires reçu\n",
    "- Utilise EXACTEMENT la période reçue (pour top/rising)\n",
    "- Ne change JAMAIS ces paramètres, même si tu penses qu'un autre critère serait mieux\n",
    "\n",
    "IMPORTANT - POUR LE HANDOFF:\n",
    "Après avoir terminé le scraping et créé le JSON structuré, tu DOIS faire un handoff vers l'agent PainAnalysisAgent \n",
    "en lui passant EXACTEMENT tes données scrapées dans le format JSON suivant.\n",
    "\n",
    "\n",
    "STRUCTURE JSON À RETOURNER:\n",
    "{json.dumps(SCRAPED_DATA_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{{\n",
    "    \"scraping_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}}\n",
    "\n",
    "Sois rigoureux, précis et respecte TOUJOURS les paramètres reçus.\"\"\",\n",
    "    tools=[\n",
    "        scrape_subreddit_posts\n",
    "    ],\n",
    "    handoffs=[agent_3_analysis],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612e230",
   "metadata": {},
   "source": [
    "#### Agent 3 : Analyse et Synthèse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2f5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_3_analysis = Agent(\n",
    "    name=\"PainAnalysisAgent\",\n",
    "    instructions=f\"\"\"Tu es un expert analyste spécialisé dans l'identification des problèmes et \n",
    "    frustrations récurrents des utilisateurs.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Analyser le sentiment et l’intensité émotionnelle de chaque post et commentaire \n",
    "2. Identifier les douleurs récurrentes, quelle que soit la langue\n",
    "3. Calculer le score de priorité de chaque douleur avec l'outil calculate_pain_score\n",
    "4. Classer les douleurs par ordre de priorité\n",
    "5. Repérer les commentaires exceptionnels proposant des solutions concrètes\n",
    "6. Stocker ces solutions exceptionnelles dans la base SQLite avec l'outil store_exceptional_solution\n",
    "7. Fournir une analyse structurée et complète\n",
    "8. Retourner un objet JSON structuré et handoff à RecommendationsAgent\n",
    "\n",
    "Critères pour une solution exceptionnelle:\n",
    "- Score du commentaire > 10\n",
    "- Propose une solution concrète et réalisable\n",
    "- Solution détaillée et utile\n",
    "\n",
    "IMPORTANT - POUR LE HANDOFF:\n",
    "Après avoir terminé ton analyse et créé le JSON structuré, tu DOIS faire un handoff vers l'agent RecommendationsAgent \n",
    "en lui passant EXACTEMENT ton analyse dans le format JSON suivant.\n",
    "\n",
    "STRUCTURE JSON À RETOURNER:\n",
    "{json.dumps(PAIN_ANALYSIS_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{{\n",
    "    \"analysis_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}}\n",
    "\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        calculate_pain_score,\n",
    "        init_solutions_database,\n",
    "        store_exceptional_solution,\n",
    "        get_stored_solutions\n",
    "    ],\n",
    "    handoffs=[agent_4_recommendations],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173fe2c",
   "metadata": {},
   "source": [
    "#### Agent 4 : Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b67314",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_4_recommendations = Agent(\n",
    "    name=\"RecommendationsAgent\",\n",
    "    instructions=f\"\"\"Tu es un expert en business development et création d'entreprises.\n",
    "    Tu es créatif, ingénieux, réaliste et propose des solutions concrètes et réalisables.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Analyser les douleurs identifiées par l'Agent 3\n",
    "2. Pour chaque douleur identifiée, propose 3 opportunités business parmis:\n",
    "   - Solutions SaaS (priorité)\n",
    "   - Produits digitaux\n",
    "   - Création de contenu\n",
    "   - Marketing/Formation\n",
    "3. Pour chaque opportunité, précise :\n",
    "   - Le type (saas, digital_product, content, marketing, etc.)\n",
    "   - Un titre clair\n",
    "   - Une description détaillée\n",
    "   - Le niveau de complexité\n",
    "   - Le coût estimé\n",
    "   - Le temps de développement estimé\n",
    "4. Classer les opportunités business selon leur potentiel (rentabilité + faisabilité)\n",
    "5. Présente le tout de façon claire et conversationnelle dans le chat (pas en JSON brut)\n",
    "6. Proposer les exports disponibles avec les différents outils :\n",
    "   - export_final_report: Rapport complet (analyse + recommandations)\n",
    "   - export_exceptional_solutions: Seulement les commentaires exceptionnels\n",
    "   - export_both_reports: Les deux rapports\n",
    "7. Rester disponible pour répondre aux questions de l'utilisateur ou générer un export\n",
    "\n",
    "\n",
    "FORMAT DE RÉPONSE :\n",
    "- Pour chaque douleur, liste les 3 opportunités business proposées, avec leurs détails\n",
    "- À la fin, propose les options d'export disponibles\n",
    "- Invite l'utilisateur à demander un export, poser des questions, ou relancer une analyse sur un autre subreddit\n",
    "\n",
    "Exemple de structure :\n",
    "\"Pour la douleur : [Nom de la douleur]\n",
    "1. [Opportunité 1] (SaaS) - Complexité: [niveau], Coût: [estimation], Temps: [estimation]\n",
    "   Description : ...\n",
    "2. [Opportunité 2] (Produit digital) - ...\n",
    "   Description : ...\n",
    "3. [Opportunité 3] (Marketing/Formation) - ...\n",
    "   Description : ...\n",
    "\n",
    "[... Répéter pour chaque douleur identifiée ...]\n",
    "\n",
    "📊 EXPORTS DISPONIBLES :\n",
    "- Rapport complet (PDF/CSV/TXT)\n",
    "- Solutions exceptionnelles uniquement\n",
    "- Les deux rapports\n",
    "\n",
    "Que souhaitez-vous faire ? Voulez-vous un export, avez-vous des questions, ou souhaitez-vous analyser un autre subreddit ?\"\n",
    "\n",
    "RÈGLE :\n",
    "- Reste conversationnel et disponible pour continuer l'échange.\n",
    "- Si l'utilisateur veut analyser un autre subreddit, transfère la main à l'Agent 1 (UserInteractionAgent) et explique-le clairement à l'utilisateur.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "    tools=[\n",
    "        get_stored_solutions,\n",
    "        export_final_report,\n",
    "        export_exceptional_solutions,\n",
    "        export_both_reports\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d480e",
   "metadata": {},
   "source": [
    "# TEST Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d758295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface Gradio avec handoff configuré dans les agents\n",
    "async def reddit_analysis_workflow(message, history):\n",
    "    \"\"\"\n",
    "    Workflow complet d'analyse Reddit avec handoff configuré dans les agents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with trace(\"reddit_analysis_workflow\"):\n",
    "            # Construire le contexte avec l'historique\n",
    "            full_context = \"\"\n",
    "            if history:\n",
    "                for human, assistant in history:\n",
    "                    full_context += f\"Humain: {human}\\nAssistant: {assistant}\\n\"\n",
    "            \n",
    "            # Ajouter le message actuel\n",
    "            full_context += f\"Humain: {message}\\nAssistant: \"\n",
    "            \n",
    "            # Lancer l'Agent 1 qui handoff automatiquement vers les autres\n",
    "            result = await Runner.run(agent_1_interaction, full_context)\n",
    "            return result.final_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Erreur dans le workflow: {str(e)}\"\n",
    "\n",
    "# Interface Gradio\n",
    "interface = gr.ChatInterface(\n",
    "    fn=reddit_analysis_workflow,\n",
    "    title=\"🤖 Analyse Reddit - Identification des Douleurs Utilisateurs\",\n",
    "    description=\"Découvrez les problèmes récurrents sur Reddit et trouvez des opportunités business\",\n",
    "    examples=[\n",
    "        [\"Je veux analyser le subreddit 'entrepreneur' pour identifier les problèmes des entrepreneurs\"],\n",
    "        [\"Analysez 'programming' pour voir les frustrations des développeurs\"],\n",
    "        [\"Je cherche des opportunités dans le subreddit 'smallbusiness'\"]\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Lancer l'interface avec port automatique\n",
    "interface.launch(share=False, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Junca\\AppData\\Local\\Temp\\ipykernel_10600\\3051357678.py:71: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cellule pour l'interface Gradio corrigée\n",
    "def create_reddit_analysis_interface():\n",
    "    \"\"\"\n",
    "    Crée l'interface Gradio avec paramètres configurables\n",
    "    \"\"\"\n",
    "    with gr.Blocks(title=\"🤖 Analyse Reddit - Identification des Douleurs Utilisateurs\") as demo:\n",
    "        gr.Markdown(\"# 🤖 Analyse Reddit - Identification des Douleurs Utilisateurs\")\n",
    "        gr.Markdown(\"Découvrez les problèmes récurrents sur Reddit et trouvez des opportunités business\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### 📋 Paramètres d'analyse\")\n",
    "                \n",
    "                # Subreddit\n",
    "                subreddit_input = gr.Textbox(\n",
    "                    label=\"Subreddit à analyser\",\n",
    "                    placeholder=\"ex: entrepreneur, programming, smallbusiness\",\n",
    "                    info=\"Entrez le nom du subreddit (sans le 'r/')\"\n",
    "                )\n",
    "                \n",
    "                # Critère de tri\n",
    "                sort_criteria = gr.Dropdown(\n",
    "                    choices=[\"top\", \"new\", \"hot\", \"best\", \"rising\"],\n",
    "                    value=\"top\",\n",
    "                    label=\"Critère de tri\",\n",
    "                    info=\"Top: meilleur score | New: plus récents | Hot: populaire | Best: pertinence | Rising: en hausse\"\n",
    "                )\n",
    "                \n",
    "                # Nombre de posts\n",
    "                num_posts = gr.Slider(\n",
    "                    minimum=5,\n",
    "                    maximum=50,\n",
    "                    value=10,\n",
    "                    step=5,\n",
    "                    label=\"Nombre de posts à analyser\",\n",
    "                    info=\"Entre 5 et 50 posts\"\n",
    "                )\n",
    "                \n",
    "                # Nombre de commentaires par post\n",
    "                comments_limit = gr.Slider(\n",
    "                    minimum=5,\n",
    "                    maximum=50,\n",
    "                    value=10,\n",
    "                    step=5,\n",
    "                    label=\"Commentaires par post\",\n",
    "                    info=\"Entre 5 et 50 commentaires\"\n",
    "                )\n",
    "                \n",
    "                # Période (pour top/rising)\n",
    "                time_filter = gr.Dropdown(\n",
    "                    choices=[\"hour\", \"day\", \"week\", \"month\", \"year\", \"all\"],\n",
    "                    value=\"month\",\n",
    "                    label=\"Période d'analyse\",\n",
    "                    info=\"Applicable pour 'top' et 'rising' uniquement\"\n",
    "                )\n",
    "                \n",
    "                # Bouton de lancement\n",
    "                analyze_btn = gr.Button(\"�� Lancer l'analyse\", variant=\"primary\", size=\"lg\")\n",
    "                \n",
    "                # Statut\n",
    "                status_text = gr.Textbox(\n",
    "                    label=\"Statut\",\n",
    "                    value=\"Prêt à analyser\",\n",
    "                    interactive=False\n",
    "                )\n",
    "            \n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"### 💬 Conversation avec l'agent\")\n",
    "                \n",
    "                # Interface de chat\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Conversation\",\n",
    "                    height=500,\n",
    "                    show_label=False\n",
    "                )\n",
    "                \n",
    "                # Zone de saisie\n",
    "                msg = gr.Textbox(\n",
    "                    label=\"Message\",\n",
    "                    placeholder=\"Tapez votre message ou cliquez sur 'Lancer l'analyse'\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                # Boutons d'action\n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"📤 Envoyer\", variant=\"secondary\")\n",
    "                    clear_btn = gr.Button(\"��️ Effacer\", variant=\"stop\")\n",
    "        \n",
    "        # Fonction pour lancer l'analyse avec les paramètres\n",
    "        def launch_analysis_with_params(subreddit, sort_crit, posts, comments, period):\n",
    "            \"\"\"\n",
    "            Lance l'analyse avec les paramètres sélectionnés\n",
    "            \"\"\"\n",
    "            if not subreddit.strip():\n",
    "                return \"❌ Veuillez entrer un nom de subreddit\", \"Erreur: Subreddit manquant\"\n",
    "            \n",
    "            # Construire le message initial avec les paramètres\n",
    "            initial_message = f\"\"\"Je veux analyser le subreddit '{subreddit}' avec les paramètres suivants :\n",
    "- Critère de tri : {sort_crit}\n",
    "- Nombre de posts : {posts}\n",
    "- Commentaires par post : {comments}\n",
    "- Période : {period}\n",
    "\n",
    "Pouvez-vous lancer l'analyse avec ces paramètres ?\"\"\"\n",
    "            \n",
    "            return initial_message, f\"Analyse en cours pour r/{subreddit}...\"\n",
    "        \n",
    "        # Fonction de chat avec historique - CORRIGÉE\n",
    "        async def chat_with_params(message, history, subreddit, sort_crit, posts, comments, period):\n",
    "            \"\"\"\n",
    "            Fonction de chat qui utilise les paramètres de l'interface\n",
    "            \"\"\"\n",
    "            try:\n",
    "                with trace(\"chat_with_params\"):\n",
    "                    # Construire le contexte avec l'historique\n",
    "                    full_context = \"\"\n",
    "                    if history:\n",
    "                        for human, assistant in history:\n",
    "                            full_context += f\"Humain: {human}\\nAssistant: {assistant}\\n\"\n",
    "                    \n",
    "                    # Ajouter le message actuel\n",
    "                    full_context += f\"Humain: {message}\\nAssistant: \"\n",
    "                    \n",
    "                    # Lancer l'Agent 1 qui handoff automatiquement vers les autres\n",
    "                    result = await Runner.run(agent_1_interaction, full_context)\n",
    "                    \n",
    "                    # Retourner le format correct pour Gradio : (humain, assistant)\n",
    "                    return history + [[message, result.final_output]]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"❌ Erreur dans le workflow: {str(e)}\"\n",
    "                return history + [[message, error_msg]]\n",
    "        \n",
    "        # Fonction pour effacer le chat\n",
    "        def clear_chat():\n",
    "            return [], \"Prêt à analyser\"\n",
    "        \n",
    "        # Événements\n",
    "        analyze_btn.click(\n",
    "            fn=launch_analysis_with_params,\n",
    "            inputs=[subreddit_input, sort_criteria, num_posts, comments_limit, time_filter],\n",
    "            outputs=[msg, status_text]\n",
    "        )\n",
    "        \n",
    "        send_btn.click(\n",
    "            fn=chat_with_params,\n",
    "            inputs=[msg, chatbot, subreddit_input, sort_criteria, num_posts, comments_limit, time_filter],\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=clear_chat,\n",
    "            outputs=[chatbot, status_text]\n",
    "        )\n",
    "        \n",
    "        # Exemples\n",
    "        gr.Markdown(\"### �� Exemples de subreddits populaires\")\n",
    "        gr.Markdown(\"- **entrepreneur** - Problèmes d'entrepreneurs\")\n",
    "        gr.Markdown(\"- **programming** - Frustrations des développeurs\")\n",
    "        gr.Markdown(\"- **smallbusiness** - Défis des petites entreprises\")\n",
    "        gr.Markdown(\"- **freelance** - Problèmes des freelances\")\n",
    "        gr.Markdown(\"- **startup** - Défis des startups\")\n",
    "        \n",
    "        return demo\n",
    "\n",
    "# Créer et lancer l'interface\n",
    "interface = create_reddit_analysis_interface()\n",
    "interface.launch(share=False, quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test Kernel",
   "language": "python",
   "name": "test-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
