{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64df690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import sqlite3\n",
    "import praw\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from agents import Agent, Runner, function_tool, trace, WebSearchTool\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a3af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = \"RedditScraper/1.0\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "Path(\"temp\").mkdir(exist_ok=True)\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ef1b8",
   "metadata": {},
   "source": [
    "# Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6257a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les paramètres utilisateur\n",
    "USER_PARAMS_STRUCTURE = {\n",
    "    \"subreddit_name\": \"string\",  # Nom du subreddit\n",
    "    \"num_posts\": 10,             # Nombre de posts à scraper (max 50)\n",
    "    \"comments_limit\": 10,        # Nombre de commentaires par post (max 50)\n",
    "    \"sort_criteria\": \"top\",      # top, new, hot, best, rising\n",
    "    \"time_filter\": \"month\",      # hour, day, week, month, year, all (pour top/rising)\n",
    "    \"confirmed\": True,           # Validation utilisateur\n",
    "    \"timestamp\": \"2024-01-01 12:00:00\",  # Date/heure de la demande\n",
    "    \"user_preferences\": {        # Préférences utilisateur\n",
    "        \"language\": \"auto\",      # Langue d'analyse\n",
    "        \"export_format\": \"all\"   # Format d'export préféré\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les données scrapées\n",
    "SCRAPED_DATA_STRUCTURE = {\n",
    "    \"scraping_success\": True,           # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"posts_count\": 10,                  # Nombre de posts récupérés\n",
    "    \"total_comments\": 100,              # Total des commentaires\n",
    "    \"scraped_at\": \"2024-01-01 12:00:00\", # Date/heure du scraping\n",
    "    \"parameters_used\": {                # Paramètres utilisés\n",
    "        \"sort_criteria\": \"top\",         # Critère de tri\n",
    "        \"time_filter\": \"month\",         # Filtre temporel\n",
    "        \"comments_limit\": 10            # Limite de commentaires\n",
    "    },\n",
    "    \"posts\": [                          # Liste des posts\n",
    "        {\n",
    "            \"title\": \"string\",          # Titre du post\n",
    "            \"author\": \"string\",         # Auteur\n",
    "            \"score\": 100,               # Score (upvotes)\n",
    "            \"upvote_ratio\": 0.95,       # Ratio upvote\n",
    "            \"num_comments\": 50,         # Nombre de commentaires\n",
    "            \"created_utc\": \"2024-01-01 10:00:00\", # Date création\n",
    "            \"url\": \"https://reddit.com/...\", # URL du post\n",
    "            \"selftext\": \"string\",       # Contenu du post\n",
    "            \"id\": \"string\",             # ID du post\n",
    "            \"comments\": [               # Liste des commentaires\n",
    "                {\n",
    "                    \"author\": \"string\", # Auteur du commentaire\n",
    "                    \"body\": \"string\",   # Contenu du commentaire\n",
    "                    \"score\": 10,        # Score du commentaire\n",
    "                    \"created_utc\": \"2024-01-01 10:30:00\", # Date création\n",
    "                    \"id\": \"string\"      # ID du commentaire\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None               # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67583e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour l'analyse des douleurs\n",
    "PAIN_ANALYSIS_STRUCTURE = {\n",
    "    \"analysis_success\": True,           # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit analysé\n",
    "    \"posts_analyzed\": 10,               # Nombre de posts analysés\n",
    "    \"total_comments_analyzed\": 100,     # Total des commentaires analysés\n",
    "    \"analysis_timestamp\": \"2024-01-01 12:00:00\", # Date/heure de l'analyse\n",
    "    \"pain_categories\": [                # Liste des catégories de douleurs\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur (ex: \"technique\", \"financier\")\n",
    "            \"frequency\": 5,             # Nombre de posts mentionnant cette douleur\n",
    "            \"avg_upvotes\": 25.5,        # Moyenne des upvotes\n",
    "            \"avg_comments\": 12.3,       # Moyenne des commentaires\n",
    "            \"avg_intensity\": 7.2,       # Intensité émotionnelle moyenne (1-10)\n",
    "            \"score\": 45.6,              # Score de priorité calculé\n",
    "            \"rank\": 1,                  # Rang dans le classement\n",
    "            \"descriptions\": [           # Exemples de descriptions\n",
    "                \"Description 1\",\n",
    "                \"Description 2\"\n",
    "            ],\n",
    "            \"posts_affected\": [         # IDs des posts concernés\n",
    "                \"post_id_1\",\n",
    "                \"post_id_2\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_pains\": [                   # Top 3 des douleurs prioritaires\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"score\": 45.6,\n",
    "            \"frequency\": 5,\n",
    "            \"avg_intensity\": 7.2,\n",
    "            \"summary\": \"Résumé de la douleur\"\n",
    "        }\n",
    "    ],\n",
    "    \"overall_summary\": \"string\",        # Résumé global de l'analyse\n",
    "    \"solutions_found\": [               # Commentaires proposant des solutions\n",
    "        {\n",
    "            \"comment_id\": \"string\",\n",
    "            \"post_id\": \"string\",\n",
    "            \"author\": \"string\",\n",
    "            \"solution_text\": \"string\",\n",
    "            \"score\": 10\n",
    "        }\n",
    "    ],\n",
    "    \"error_message\": None              # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c349f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure JSON pour les recommandations\n",
    "RECOMMENDATIONS_STRUCTURE = {\n",
    "    \"recommendations_success\": True,    # Booléen\n",
    "    \"subreddit\": \"string\",              # Nom du subreddit\n",
    "    \"analysis_summary\": \"string\",       # Résumé de l'analyse de l'Agent 3\n",
    "    \"recommendations_timestamp\": \"2024-01-01 12:00:00\", # Date/heure\n",
    "    \"business_opportunities\": [         # Opportunités business identifiées\n",
    "        {\n",
    "            \"pain_type\": \"string\",      # Type de douleur\n",
    "            \"pain_score\": 45.6,         # Score de la douleur\n",
    "            \"opportunity_rank\": 1,      # Rang de l'opportunité\n",
    "            \"solutions\": [              # 3 solutions proposées\n",
    "                {\n",
    "                    \"type\": \"saas\",     # saas, digital_product, content, marketing\n",
    "                    \"title\": \"string\",  # Titre de la solution\n",
    "                    \"description\": \"string\", # Description détaillée\n",
    "                    \"complexity\": \"low\", # low, medium, high\n",
    "                    \"estimated_cost\": \"string\", # Coût estimé\n",
    "                    \"time_to_market\": \"string\" # Temps de développement\n",
    "                }\n",
    "            ],\n",
    "            \"market_size\": \"string\",    # Taille du marché estimée\n",
    "            \"competition_level\": \"low\"  # low, medium, high\n",
    "        }\n",
    "    ],\n",
    "    \"top_3_opportunities\": [           # Top 3 des meilleures opportunités\n",
    "        {\n",
    "            \"pain_type\": \"string\",\n",
    "            \"pain_score\": 45.6,\n",
    "            \"best_solution\": {\n",
    "                \"type\": \"saas\",\n",
    "                \"title\": \"string\",\n",
    "                \"description\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"export_available\": True,          # Si les exports sont disponibles\n",
    "    \"stored_solutions_count\": 5,       # Nombre de solutions stockées en SQLite\n",
    "    \"error_message\": None              # Message d'erreur si échec\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5757db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def init_solutions_database() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Initialise la base de données SQLite pour stocker les commentaires avec solutions\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut de l'initialisation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Créer la table des solutions\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS solutions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                comment_id TEXT UNIQUE,\n",
    "                post_id TEXT,\n",
    "                author TEXT,\n",
    "                solution_text TEXT,\n",
    "                score INTEGER,\n",
    "                pain_type TEXT,\n",
    "                intensity INTEGER,\n",
    "                subreddit TEXT,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"✅ Base de données initialisée avec succès\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def store_exceptional_solution(comment_data: str, pain_type: str, intensity: int) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Stocke un commentaire exceptionnel proposant une solution\n",
    "    \n",
    "    Args:\n",
    "        comment_data: Données du commentaire en JSON string\n",
    "        pain_type: Type de douleur identifiée\n",
    "        intensity: Intensité émotionnelle (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut du stockage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comment = json.loads(comment_data)\n",
    "        \n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO solutions \n",
    "            (comment_id, post_id, author, solution_text, score, pain_type, intensity, subreddit)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            comment.get('id'),\n",
    "            comment.get('post_id'),\n",
    "            comment.get('author'),\n",
    "            comment.get('body'),\n",
    "            comment.get('score', 0),\n",
    "            pain_type,\n",
    "            intensity,\n",
    "            comment.get('subreddit', 'unknown')\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"✅ Solution stockée: {comment.get('author', 'Unknown')}\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def get_stored_solutions(subreddit: str = None, limit: int = 10) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Récupère les solutions stockées en base\n",
    "    \n",
    "    Args:\n",
    "        subreddit: Filtrer par subreddit (optionnel)\n",
    "        limit: Nombre maximum de solutions à récupérer\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les solutions récupérées\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect('solutions.db')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if subreddit:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                WHERE subreddit = ? \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (subreddit, limit))\n",
    "        else:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM solutions \n",
    "                ORDER BY score DESC, created_at DESC \n",
    "                LIMIT ?\n",
    "            ''', (limit,))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        solutions = []\n",
    "        for row in rows:\n",
    "            solutions.append({\n",
    "                \"id\": row[0],\n",
    "                \"comment_id\": row[1],\n",
    "                \"post_id\": row[2],\n",
    "                \"author\": row[3],\n",
    "                \"solution_text\": row[4],\n",
    "                \"score\": row[5],\n",
    "                \"pain_type\": row[6],\n",
    "                \"intensity\": row[7],\n",
    "                \"subreddit\": row[8],\n",
    "                \"created_at\": row[9]\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"solutions\": solutions,\n",
    "            \"count\": len(solutions)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7b37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def export_final_report(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final avec analyse et recommandations\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parser les données JSON\n",
    "        analysis = json.loads(analysis_data)\n",
    "        recommendations = json.loads(recommendations_data)\n",
    "        \n",
    "        filename = f\"rapport_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        if format_type == \"pdf\":\n",
    "            return export_analysis_to_pdf(analysis, recommendations, filename)\n",
    "        elif format_type == \"csv\":\n",
    "            return export_analysis_to_csv(analysis, recommendations, filename)\n",
    "        elif format_type == \"txt\":\n",
    "            return export_analysis_to_txt(analysis, recommendations, filename)\n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_exceptional_solutions(format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte uniquement les solutions exceptionnelles stockées en base\n",
    "    \n",
    "    Args:\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et le chemin du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupérer toutes les solutions\n",
    "        solutions_result = get_stored_solutions(limit=1000)\n",
    "        \n",
    "        if not solutions_result[\"success\"]:\n",
    "            return {\"success\": False, \"error\": \"Impossible de récupérer les solutions\"}\n",
    "        \n",
    "        solutions = solutions_result[\"solutions\"]\n",
    "        filename = f\"solutions_exceptionnelles_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        export_dir = Path(\"exports\")\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        if format_type == \"txt\":\n",
    "            filepath = export_dir / f\"{filename}.txt\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"SOLUTIONS EXCEPTIONNELLES REDDIT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Nombre de solutions: {len(solutions)}\\n\\n\")\n",
    "                \n",
    "                for i, solution in enumerate(solutions, 1):\n",
    "                    f.write(f\"SOLUTION {i}:\\n\")\n",
    "                    f.write(f\"Auteur: {solution['author']}\\n\")\n",
    "                    f.write(f\"Subreddit: r/{solution['subreddit']}\\n\")\n",
    "                    f.write(f\"Score: {solution['score']}\\n\")\n",
    "                    f.write(f\"Type de douleur: {solution['pain_type']}\\n\")\n",
    "                    f.write(f\"Intensité: {solution['intensity']}/10\\n\")\n",
    "                    f.write(f\"Date: {solution['created_at']}\\n\")\n",
    "                    f.write(f\"Solution:\\n{solution['solution_text']}\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en TXT: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"csv\":\n",
    "            import csv\n",
    "            filepath = export_dir / f\"{filename}.csv\"\n",
    "            with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['Auteur', 'Subreddit', 'Score', 'Type de douleur', 'Intensité', 'Date', 'Solution'])\n",
    "                for solution in solutions:\n",
    "                    writer.writerow([\n",
    "                        solution['author'],\n",
    "                        solution['subreddit'],\n",
    "                        solution['score'],\n",
    "                        solution['pain_type'],\n",
    "                        solution['intensity'],\n",
    "                        solution['created_at'],\n",
    "                        solution['solution_text']\n",
    "                    ])\n",
    "            \n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en CSV: {filepath}\"}\n",
    "        \n",
    "        elif format_type == \"pdf\":\n",
    "            from reportlab.lib.pagesizes import letter\n",
    "            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "            from reportlab.lib.styles import getSampleStyleSheet\n",
    "            \n",
    "            filepath = export_dir / f\"{filename}.pdf\"\n",
    "            doc = SimpleDocTemplate(str(filepath), pagesize=letter)\n",
    "            styles = getSampleStyleSheet()\n",
    "            story = []\n",
    "            \n",
    "            title = Paragraph(f\"SOLUTIONS EXCEPTIONNELLES REDDIT\", styles['Title'])\n",
    "            story.append(title)\n",
    "            story.append(Spacer(1, 12))\n",
    "            \n",
    "            for i, solution in enumerate(solutions, 1):\n",
    "                story.append(Paragraph(f\"<b>Solution {i}:</b>\", styles['Heading2']))\n",
    "                story.append(Paragraph(f\"Auteur: {solution['author']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Subreddit: r/{solution['subreddit']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Score: {solution['score']}\", styles['Normal']))\n",
    "                story.append(Paragraph(f\"Solution: {solution['solution_text']}\", styles['Normal']))\n",
    "                story.append(Spacer(1, 12))\n",
    "            \n",
    "            doc.build(story)\n",
    "            return {\"success\": True, \"filepath\": str(filepath), \"message\": f\"✅ Solutions exportées en PDF: {filepath}\"}\n",
    "        \n",
    "        else:\n",
    "            return {\"success\": False, \"error\": \"Format non supporté\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def export_both_reports(analysis_data: str, recommendations_data: str, format_type: str = \"pdf\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Exporte le rapport final ET les solutions exceptionnelles\n",
    "    \n",
    "    Args:\n",
    "        analysis_data: Données de l'analyse (Agent 3) en JSON string\n",
    "        recommendations_data: Données des recommandations (Agent 4) en JSON string\n",
    "        format_type: Format d'export (pdf, csv, txt)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le statut et les chemins des fichiers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Exporter le rapport final\n",
    "        report_result = export_final_report(analysis_data, recommendations_data, format_type)\n",
    "        \n",
    "        # Exporter les solutions exceptionnelles\n",
    "        solutions_result = export_exceptional_solutions(format_type)\n",
    "        \n",
    "        if report_result[\"success\"] and solutions_result[\"success\"]:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"report_file\": report_result[\"filepath\"],\n",
    "                \"solutions_file\": solutions_result[\"filepath\"],\n",
    "                \"message\": \"✅ Rapport final et solutions exportés\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Erreur rapport: {report_result.get('error', 'OK')}, Erreur solutions: {solutions_result.get('error', 'OK')}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15a1be",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dabefe",
   "metadata": {},
   "source": [
    "#### Subreddit check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0139a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def check_subreddit_exists(subreddit_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Vérifie si un subreddit existe via l'API PRAW\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit (sans le 'r/')\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les informations du subreddit\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Utiliser PRAW pour accéder au subreddit\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Essayer d'accéder aux informations du subreddit\n",
    "        # Cela va lever une exception si le subreddit n'existe pas\n",
    "        subreddit_info = {\n",
    "            \"exists\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"subscribers\": subreddit.subscribers,\n",
    "            \"description\": subreddit.public_description,\n",
    "            \"title\": subreddit.title,\n",
    "            \"is_private\": subreddit.subreddit_type == 'private',\n",
    "            \"created_utc\": datetime.fromtimestamp(subreddit.created_utc).strftime('%Y-%m-%d'),\n",
    "            \"url\": f\"https://reddit.com/r/{subreddit_name}\"\n",
    "        }\n",
    "        \n",
    "        return subreddit_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def calculate_pain_score(frequency: int, avg_upvotes: float, avg_comments: float, avg_intensity: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcule le score de priorité d'une douleur utilisateur\n",
    "    \n",
    "    Args:\n",
    "        frequency: Nombre de posts mentionnant cette douleur\n",
    "        avg_upvotes: Moyenne des upvotes des posts concernés\n",
    "        avg_comments: Moyenne des commentaires des posts concernés\n",
    "        avg_intensity: Intensité émotionnelle moyenne (1-10)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec le score calculé et les détails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Formule de scoring\n",
    "        score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (avg_intensity * 0.3)\n",
    "        \n",
    "        # Normaliser l'intensité (1-10 vers 0-100)\n",
    "        normalized_intensity = avg_intensity * 10\n",
    "        \n",
    "        # Calculer les composantes pour le détail\n",
    "        frequency_component = frequency * 0.4\n",
    "        upvotes_component = avg_upvotes * 0.2\n",
    "        comments_component = avg_comments * 0.1\n",
    "        intensity_component = normalized_intensity * 0.3\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"total_score\": round(score, 2),\n",
    "            \"components\": {\n",
    "                \"frequency_component\": round(frequency_component, 2),\n",
    "                \"upvotes_component\": round(upvotes_component, 2),\n",
    "                \"comments_component\": round(comments_component, 2),\n",
    "                \"intensity_component\": round(intensity_component, 2)\n",
    "            },\n",
    "            \"formula\": \"score = (frequency * 0.4) + (avg_upvotes * 0.2) + (avg_comments * 0.1) + (intensity * 0.3)\",\n",
    "            \"input_values\": {\n",
    "                \"frequency\": frequency,\n",
    "                \"avg_upvotes\": avg_upvotes,\n",
    "                \"avg_comments\": avg_comments,\n",
    "                \"avg_intensity\": avg_intensity\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915eade",
   "metadata": {},
   "source": [
    "#### Reddit Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13721b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def scrape_subreddit_posts(subreddit_name: str, num_posts: int = 10, sort_criteria: str = \"top\", comments_limit: int = 10, time_filter: str = \"month\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrape les posts d'un subreddit selon les paramètres donnés\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nom du subreddit\n",
    "        num_posts: Nombre de posts à récupérer (max 50)\n",
    "        sort_criteria: Critère de tri (top, new, hot, best, rising)\n",
    "        comments_limit: Nombre de commentaires par post (max 50)\n",
    "        time_filter: Période pour top/rising (hour, day, week, month, year, all)\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec les posts scrapés et métadonnées\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Limiter les valeurs\n",
    "        num_posts = min(num_posts, 50)\n",
    "        comments_limit = min(comments_limit, 50)\n",
    "        \n",
    "        # Vérifier que le subreddit existe\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Vérifier si le critère demandé existe\n",
    "        original_criteria = sort_criteria\n",
    "        fallback_used = False\n",
    "        \n",
    "        # Tester seulement le critère demandé\n",
    "        if sort_criteria == \"top\" and hasattr(subreddit, 'top'):\n",
    "            sort_method = subreddit.top\n",
    "        elif sort_criteria == \"new\" and hasattr(subreddit, 'new'):\n",
    "            sort_method = subreddit.new\n",
    "        elif sort_criteria == \"hot\" and hasattr(subreddit, 'hot'):\n",
    "            sort_method = subreddit.hot\n",
    "        elif sort_criteria == \"best\" and hasattr(subreddit, 'best'):\n",
    "            sort_method = subreddit.best\n",
    "        elif sort_criteria == \"rising\" and hasattr(subreddit, 'rising'):\n",
    "            sort_method = subreddit.rising\n",
    "        else:\n",
    "            # Fallback sur \"new\" si le critère demandé n'existe pas\n",
    "            sort_criteria = \"new\"\n",
    "            sort_method = subreddit.new\n",
    "            fallback_used = True\n",
    "        \n",
    "        # Récupérer les posts\n",
    "        posts_data = []\n",
    "        \n",
    "        try:\n",
    "            if sort_criteria in [\"top\", \"rising\"]:\n",
    "                posts = sort_method(limit=num_posts, time_filter=time_filter)\n",
    "            else:\n",
    "                posts = sort_method(limit=num_posts)\n",
    "        except Exception as e:\n",
    "            # Si le critère échoue, essayer \"new\"\n",
    "            sort_criteria = \"new\"\n",
    "            posts = subreddit.new(limit=num_posts)\n",
    "            fallback_used = True\n",
    "        \n",
    "        for post in posts:\n",
    "            # Récupérer les commentaires\n",
    "            post.comments.replace_more(limit=5)\n",
    "            \n",
    "            comments_data = []\n",
    "            for comment in post.comments.list()[:comments_limit]:\n",
    "                if hasattr(comment, 'body') and comment.body:\n",
    "                    comments_data.append({\n",
    "                        \"author\": str(comment.author) if comment.author else \"[deleted]\",\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"created_utc\": datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        \"id\": comment.id\n",
    "                    })\n",
    "            \n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": str(post.author) if post.author else \"[deleted]\",\n",
    "                \"score\": post.score,\n",
    "                \"upvote_ratio\": post.upvote_ratio,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "                \"selftext\": post.selftext[:1000] + \"...\" if len(post.selftext) > 1000 else post.selftext,\n",
    "                \"comments\": comments_data,\n",
    "                \"id\": post.id\n",
    "            }\n",
    "            \n",
    "            posts_data.append(post_data)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"sort_criteria\": sort_criteria,\n",
    "            \"original_criteria\": original_criteria,\n",
    "            \"time_filter\": time_filter,\n",
    "            \"posts_count\": len(posts_data),\n",
    "            \"comments_limit_per_post\": comments_limit,\n",
    "            \"posts\": posts_data,\n",
    "            \"scraped_at\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"fallback_used\": fallback_used\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"subreddit\": subreddit_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e88e8",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b193611",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_0 = \"\"\" Tu es le RouterAgent, le chef d'orchestre du système d'analyse Reddit.\n",
    "Tu es le SEUL point d'entrée pour toutes les conversations utilisateur.\n",
    "Ta mission est d'analyser un subreddit pour détecter les problèmes/frustrations récurrents.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Saluer l'utilisateur avec politesse et professionnalisme et expliquer ta mission.\n",
    "2. Analyser chaque demande utilisateur et décider de la meilleure action\n",
    "\n",
    "Si l'utilisateur te demande de lancer une nouvelle analyse, tu dois:\n",
    "\n",
    "1. Demander le subreddit à analyser\n",
    "2. Vérifier que le subreddit existe avec l'outil check_subreddit_exist\n",
    "3. Interagir avec l'utilisateur pour collecter les paramètres\n",
    "4. Expliquer les 5 critères de tri Reddit ainsi que les paramètres si nécessaire\n",
    "5. Si l'utilisateur confirme sans donner de paramètres, tu dois proposer les paramètres par défaut\n",
    "6. Confirmer avec l'utilisateur\n",
    "7. Handoff vers le WorkflowManager pour gérer la nouvelle analyse\n",
    "8. Une fois les résultats finaux obtenus, présenter les résultats à l'utilisateur\n",
    "\n",
    "\n",
    "Si l'utilisateur demande d'exporter le rapport, tu dois:\n",
    "1. Vérifier que l'analyse est terminée et que tu as le rapport final\n",
    "2. Utiliser les tools d'export appropriés (get_stored_solutions,\n",
    "        export_final_report,\n",
    "        export_exceptional_solutions,\n",
    "        export_both_reports)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PARAMÈTRES PAR DÉFAUT:\n",
    "- Nombre de posts: 5\n",
    "- Nombre de commentaires par post: 5\n",
    "- Critère: \"top\"\n",
    "- Période: \"month\"\n",
    "\n",
    "Si on te pose des questions sur l'explication des critères de tri: \n",
    "   - \"Top\" → les posts avec le meilleur score sur une période (votes positifs - négatifs)\n",
    "   - \"New\" → les posts les plus récents (ordre chronologique)\n",
    "   - \"Hot\" → mélange du score + fraîcheur (post récent et populaire)\n",
    "   - \"Best\" → pertinence + vote + réponse\n",
    "   - \"Rising\" → posts récents qui gagnent rapidement en popularité\n",
    "\n",
    "INSTRUCTIONS HANDOFF:\n",
    "- Quand tu handoff vers WorkflowManager, précise: \"Je transfère vers le WorkflowManager pour gérer votre nouvelle analyse\"\n",
    "- Attends TOUJOURS le retour du WorkflowManager avant de continuer\n",
    "- Présente les résultats de façon claire et conversationnelle\n",
    "\n",
    "PHRASE D'ACCUEIL:\n",
    "\"Bonjour ! Je suis votre assistant d'analyse Reddit. Je peux analyser n'importe quel subreddit pour identifier les problèmes récurrents des utilisateurs et vous proposer des opportunités business. Quel subreddit souhaitez-vous analyser ?\"\n",
    "\n",
    "RÈGLE ABSOLUE: Tu es le seul agent à parler directement à l'utilisateur au début et à la fin de chaque workflow.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3f2cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\" Tu es le workflow manager, le gestionnaire du workflow d'analyse Reddit.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Recevoir les demandes d'analyse du RouterAgent\n",
    "2 . Utiliser les tools en séquence pour l'analyse complète\n",
    "3. Retourner les résultats à RouterAgent\n",
    "\n",
    "PROCESSUS:\n",
    "4. Utiliser les tools dans l'ordre:\n",
    "   - reddit_scraper_tool\n",
    "   - pain_analyzer_tool  \n",
    "   - report_generator_tool\n",
    "5. Une fois TOUT terminé, handoff vers RouterAgent avec les résultats complets\n",
    "\n",
    "\n",
    "STRUCTURE JSON À UTILISER:\n",
    "{json.dumps(USER_PARAMS_STRUCTURE, indent=2)}\n",
    "\n",
    "RÈGLE IMPORTANTE: Ne handoff vers RouterAgent UNIQUEMENT quand tu as les résultats complets de tous les tools.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b14887",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\" Tu es maintenant un TOOL utilisé par Worflow manager pour scraper Reddit.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Recevoir les paramètres exacts de Worflow manager (subreddit, nombre de posts, nombre de commentaires, critère de tri, période)\n",
    "2. Scraper les données avec l'outil scrape_subreddit_posts\n",
    "3. Vérifier que les données ont bien été récupérées (pas vides, structure correcte)\n",
    "4. Retourner les données structurées à Worflow manager\n",
    "\n",
    "IMPORTANT - RESPECTER LES PARAMÈTRES:\n",
    "- Utilise EXACTEMENT les paramètres reçus\n",
    "- Ne modifie JAMAIS les critères de tri\n",
    "- Vérifie que les données sont complètes\n",
    "\n",
    "STRUCTURE JSON À RETOURNER:\n",
    "{json.dumps(SCRAPED_DATA_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{\n",
    "    \"scraping_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f1476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = \"\"\" Tu es maintenant un TOOL utilisé par Worflow manager pour analyser les douleurs.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Recevoir les données scrapées d'Worflow manager\n",
    "2. Analyser sentiments et intensité émotionnelle\n",
    "3. Identifier les douleurs récurrentes\n",
    "4. Calculer les scores avec calculate_pain_score\n",
    "5. Stocker les solutions exceptionnelles avec store_exceptional_solution\n",
    "6. Retourner l'analyse structurée à Worflow manager\n",
    "\n",
    "CRITÈRES SOLUTIONS EXCEPTIONNELLES:\n",
    "- Score du commentaire > 10\n",
    "- Propose une solution concrète et réalisable\n",
    "- Solution détaillée et utile\n",
    "\n",
    "STRUCTURE JSON À RETOURNER:\n",
    "{json.dumps(PAIN_ANALYSIS_STRUCTURE, indent=2)}\n",
    "\n",
    "En cas d'erreur, retourner:\n",
    "{\n",
    "    \"analysis_success\": false,\n",
    "    \"error_message\": \"Description de l'erreur\",\n",
    "    \"subreddit\": \"nom_du_subreddit\"\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe7c2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4 = \"\"\" Tu es maintenant un TOOL utilisé par Worflow manager pour générer les recommandations.\n",
    "\n",
    "Ton rôle est de:\n",
    "1. Recevoir l'analyse des douleurs d'Worflow manager\n",
    "2. Générer 3 opportunités business par douleur\n",
    "3. Classer par potentiel (rentabilité + faisabilité)\n",
    "4. Retourner le rapport structuré à Worflow manager\n",
    "\n",
    "TYPES D'OPPORTUNITÉS:\n",
    "- Solutions SaaS (priorité)\n",
    "- Produits digitaux\n",
    "- Création de contenu\n",
    "- Marketing/Formation\n",
    "\n",
    "POUR CHAQUE OPPORTUNITÉ:\n",
    "- Type, titre, description détaillée\n",
    "- Niveau de complexité\n",
    "- Coût estimé\n",
    "- Temps de développement\n",
    "\n",
    "STRUCTURE DE RETOUR:\n",
    "Rapport conversationnel + options d'export disponibles\n",
    "\n",
    "RÈGLE: Tu retournes le rapport final à Worflow manager, qui le transmettra à Agent 0.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3cd6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0 =  Agent(\n",
    "    name=\"RouterAgent\", \n",
    "    instructions=prompt_0,\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "        check_subreddit_exists,\n",
    "        get_stored_solutions,\n",
    "        export_final_report,\n",
    "        export_exceptional_solutions,\n",
    "        export_both_reports], \n",
    "        model=\"gpt-4o-mini\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bcc915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(\n",
    "    name=\"WorkflowManager\",\n",
    "    instructions=prompt_1,\n",
    "    tools=[\n",
    "        Scraper_tool,\n",
    "        PainAnalysis_tool,\n",
    "        Recommendations_tool\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ab42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0.handoffs = [agent_1]\n",
    "agent_1.handoffs = [agent_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "453d4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2 = Agent(\n",
    "    name=\"ScrapingAgent\",\n",
    "    instructions=prompt_2,\n",
    "    tools=[\n",
    "        scrape_subreddit_posts\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "Scraper_tool = agent_2.as_tool(tool_name=\"scraper_tool\", tool_description=\"scrape a subreddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2f5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_3 = Agent(\n",
    "    name=\"PainAnalysisAgent\",\n",
    "    instructions=prompt_3,\n",
    "    tools=[\n",
    "        calculate_pain_score,\n",
    "        init_solutions_database,\n",
    "        store_exceptional_solution,\n",
    "        get_stored_solutions\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "PainAnalysis_tool = agent_3.as_tool(tool_name=\"pain_analysis_tool\", tool_description=\"analyze the pain of a subreddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b67314",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_4 = Agent(\n",
    "    name=\"RecommendationsAgent\",\n",
    "    instructions=prompt_4,\n",
    "    tools=[],\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "Recommendations_tool = agent_4.as_tool(tool_name=\"recommendations_tool\", tool_description=\"generate recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d480e",
   "metadata": {},
   "source": [
    "# TEST Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7945717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Junca\\projects\\Projet_Reddit\\Backend\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat ultra-simple avec format correct\n",
    "import gradio as gr\n",
    "from agents import Runner, trace\n",
    "\n",
    "async def simple_chat(message, history):\n",
    "    \"\"\"\n",
    "    Chat ultra-simple avec format correct\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construire le contexte\n",
    "        context = \"\"\n",
    "        if history:\n",
    "            for msg in history:\n",
    "                if isinstance(msg, dict):\n",
    "                    if msg.get(\"role\") == \"user\":\n",
    "                        context += f\"Humain: {msg.get('content', '')}\\n\"\n",
    "                    elif msg.get(\"role\") == \"assistant\":\n",
    "                        context += f\"Assistant: {msg.get('content', '')}\\n\"\n",
    "                else:\n",
    "                    # Fallback pour l'ancien format\n",
    "                    context += f\"Humain: {msg[0]}\\nAssistant: {msg[1]}\\n\"\n",
    "        \n",
    "        context += f\"Humain: {message}\\nAssistant: \"\n",
    "        \n",
    "        # Lancer l'agent\n",
    "        result = await Runner.run(agent_0, context)\n",
    "        \n",
    "        # Retourner le format correct pour Gradio\n",
    "        return result.final_output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"❌ Erreur: {str(e)}\"\n",
    "\n",
    "# Interface ultra-simple\n",
    "demo = gr.ChatInterface(\n",
    "    fn=simple_chat,\n",
    "    title=\"🤖 Chat Test\",\n",
    "    description=\"Test simple de l'architecture\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
